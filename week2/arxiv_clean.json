[
  {
    "url": "http://arxiv.org/abs/2508.00819v1",
    "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language   Models",
    "authors": [
      "Jinsong Li",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Jiaqi Wang",
      "Dahua Lin"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation."
  },
  {
    "url": "http://arxiv.org/abs/2508.00788v1",
    "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun   Handling in Large Language Models",
    "authors": [
      "Xushuo Tang",
      "Yi Ding",
      "Zhengyi Yang",
      "Yin Chen",
      "Yongrui Gu",
      "Wenke Yang",
      "Mingchen Ju",
      "Xin Cao",
      "Yongfei Liu",
      "Wenjie Zhang"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research."
  },
  {
    "url": "http://arxiv.org/abs/2508.00762v1",
    "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A   Zero-Shot Approach using LLM-Driven Code Generation",
    "authors": [
      "Atakan Site",
      "Emre Hakan Erdemir",
      "G\u00fcl\u015fen Eryi\u011fit"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paper's submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category."
  },
  {
    "url": "http://arxiv.org/abs/2508.00760v1",
    "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese   Hate Speech Detection under Cloaking Perturbations",
    "authors": [
      "Qiyao Xue",
      "Yuchen Dou",
      "Ryan Shi",
      "Xiang Lorraine Li",
      "Wei Gao"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches."
  },
  {
    "url": "http://arxiv.org/abs/2508.00757v1",
    "title": "GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction",
    "authors": [
      "Robin Armingaud",
      "Romaric Besan\u00e7on"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to the need to model complex interactions between entities across sentences. Current approaches, largely based on the ATLOP architecture, are commonly evaluated on benchmarks like DocRED and Re-DocRED. However, their performance in zero-shot or few-shot settings remains largely underexplored due to the task's complexity. Recently, the GLiNER model has shown that a compact NER model can outperform much larger Large Language Models. With a similar motivation, we introduce GLiDRE, a new model for document-level relation extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against state-of-the-art models across various data settings on the Re-DocRED dataset. Our results demonstrate that GLiDRE achieves state-of-the-art performance in few-shot scenarios. Our code is publicly available."
  },
  {
    "url": "http://arxiv.org/abs/2508.00743v1",
    "title": "Agentic large language models improve retrieval-based radiology question   answering",
    "authors": [
      "Sebastian Wind",
      "Jeta Sopa",
      "Daniel Truhn",
      "Mahshad Lotfinia",
      "Tri-Thien Nguyen",
      "Keno Bressem",
      "Lisa Adams",
      "Mirabela Rusu",
      "Harald K\u00f6stler",
      "Gerhard Wellein",
      "Andreas Maier",
      "Soroosh Tayebi Arasteh"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility."
  },
  {
    "url": "http://arxiv.org/abs/2508.00742v1",
    "title": "Applying Psychometrics to Large Language Model Simulated Populations:   Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
    "authors": [
      "Sarah Mercer",
      "Daniel P. Martin",
      "Phil Swatton"
    ],
    "date": "2025-08-01",
    "abstract": "Title:Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents"
  },
  {
    "url": "http://arxiv.org/abs/2508.00741v1",
    "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data   Leveraging Declarative Facts in Earlier Training Data",
    "authors": [
      "Sohaib Imran",
      "Rob Lamb",
      "Peter M. Atkinson"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety."
  },
  {
    "url": "http://arxiv.org/abs/2508.00719v1",
    "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA",
    "authors": [
      "Yingxu Wang",
      "Shiqi Fan",
      "Mengzhu Wang",
      "Siwei Liu"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods."
  },
  {
    "url": "http://arxiv.org/abs/2508.00709v1",
    "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian   Common Law System",
    "authors": [
      "Shubham Kumar Nigam",
      "Balaramamahanthi Deepak Patnaik",
      "Shivam Mishra",
      "Ajay Varghese Thomas",
      "Noel Shallum",
      "Kripabandhu Ghosh",
      "Arnab Bhattacharya"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality."
  },
  {
    "url": "http://arxiv.org/abs/2508.00695v1",
    "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep   Learning and Machine Learning Approach",
    "authors": [
      "Sergio Rubio-Mart\u00edn",
      "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s",
      "Antonio Serrano-Garc\u00eda",
      "Clara Margarita Franch-Pato",
      "Arturo Crespo-\u00c1lvaro",
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like Anxiety and Adjustment Disorder. In this study, we compare the performance of various Artificial Intelligence models, including both traditional Machine Learning approaches (Random Forest, Support Vector Machine, K-nearest neighbors, Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with BERT-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods."
  },
  {
    "url": "http://arxiv.org/abs/2508.00680v1",
    "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?",
    "authors": [
      "Johannes R\u00f6misch",
      "Svetlana Gorovaia",
      "Mariia Halchynska",
      "Gleb Schmidt",
      "Ivan P. Yamshchikov"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported."
  },
  {
    "url": "http://arxiv.org/abs/2508.00679v1",
    "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical   Role-Based Queries",
    "authors": [
      "Shubham Kumar Nigam",
      "Tanmay Dubey",
      "Noel Shallum",
      "Arnab Bhattacharya"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available."
  },
  {
    "url": "http://arxiv.org/abs/2508.00675v1",
    "title": "Team \"better_call_claude\": Style Change Detection using a Sequential   Sentence Pair Classifier",
    "authors": [
      "Gleb Schmidt",
      "Johannes R\u00f6misch",
      "Mariia Halchynska",
      "Svetlana Gorovaia",
      "Ivan P. Yamshchikov"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Style change detection - identifying the points in a document where writing style shifts - remains one of the most important and challenging problems in computational authorship analysis. At PAN 2025, the shared task challenges participants to detect style switches at the most fine-grained level: individual sentences. The task spans three datasets, each designed with controlled and increasing thematic variety within documents. We propose to address this problem by modeling the content of each problem instance - that is, a series of sentences - as a whole, using a Sequential Sentence Pair Classifier (SSPC). The architecture leverages a pre-trained language model (PLM) to obtain representations of individual sentences, which are then fed into a bidirectional LSTM (BiLSTM) to contextualize them within the document. The BiLSTM-produced vectors of adjacent sentences are concatenated and passed to a multi-layer perceptron for prediction per adjacency. Building on the work of previous PAN participants classical text segmentation, the approach is relatively conservative and lightweight. Nevertheless, it proves effective in leveraging contextual information and addressing what is arguably the most challenging aspect of this year's shared task: the notorious problem of \"stylistically shallow\", short sentences that are prevalent in the proposed benchmark data. Evaluated on the official PAN-2025 test datasets, the model achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD data, respectively, outperforming not only the official random baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot performance."
  },
  {
    "url": "http://arxiv.org/abs/2508.00673v1",
    "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of   Culture in Persian Language",
    "authors": [
      "Farhan Farsi",
      "Farnaz Aghababaloo",
      "Shahriar Shariati Motlagh",
      "Parsa Ghofrani",
      "MohammadAli SadraeiJavaheri",
      "Shayan Bali",
      "Amirhossein Shabani",
      "Farbod Bijary",
      "Ghazal Zamaninejad",
      "AmirMohammad Salehoof",
      "Saeedeh Momtazi"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field."
  },
  {
    "url": "http://arxiv.org/abs/2508.00669v1",
    "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement   Techniques and Applications",
    "authors": [
      "Wenxuan Wang",
      "Zizhan Ma",
      "Meidan Ding",
      "Shiyi Zheng",
      "Shengyuan Liu",
      "Jie Liu",
      "Jiaming Ji",
      "Wenting Chen",
      "Xiang Li",
      "Linlin Shen",
      "Yixuan Yuan"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI."
  },
  {
    "url": "http://arxiv.org/abs/2508.00659v1",
    "title": "Demo: TOSense -- What Did You Just Agree to?",
    "authors": [
      "Xinzhang Chen",
      "Hassan Ali",
      "Arash Shaghaghi",
      "Salil S. Kanhere",
      "Sanjay Jha"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Online services often require users to agree to lengthy and obscure Terms of Service (ToS), leading to information asymmetry and legal risks. This paper proposes TOSense-a Chrome extension that allows users to ask questions about ToS in natural language and get concise answers in real time. The system combines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and (ii) a lightweight large language model pipeline: MiniLM for semantic retrieval and BART-encoder for answer relevance verification. To avoid expensive manual annotation, we present a novel Question Answering Evaluation Pipeline (QEP) that generates synthetic questions and verifies the correctness of answers using clustered topic matching. Experiments on five major platforms, Apple, Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of TOSense (with up to 44.5% accuracy) across varying number of topic clusters. During the demonstration, we will showcase TOSense in action. Attendees will be able to experience seamless extraction, interactive question answering, and instant indexing of new sites."
  },
  {
    "url": "http://arxiv.org/abs/2508.00619v1",
    "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language   Models",
    "authors": [
      "Shantanu Thorat",
      "Andrew Caines"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors."
  },
  {
    "url": "http://arxiv.org/abs/2508.00614v1",
    "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will   you care?",
    "authors": [
      "Lennart Meincke",
      "Ethan Mollick",
      "Lilach Mollick",
      "Dan Shapiro"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024)."
  },
  {
    "url": "http://arxiv.org/abs/2508.00605v1",
    "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource   Bengali Language",
    "authors": [
      "Farhana Haque",
      "Md. Abdur Rahman",
      "Sumon Ahmed"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Topic modeling is a Natural Language Processing (NLP) technique that is used to identify latent themes and extract topics from text corpora by grouping similar documents based on their most significant keywords. Although widely researched in English, topic modeling remains understudied in Bengali due to its morphological complexity, lack of adequate resources and initiatives. In this contribution, a novel Graph Convolutional Network (GCN) based model called GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input vectors of documents as nodes in the graph, which GCN uses to produce semantically rich embeddings. The embeddings are then decomposed using Non-negative Matrix Factorization (NMF) to get the topical representations of the underlying themes of the text corpus. This study compares the proposed model against a wide range of Bengali topic modeling techniques, from traditional methods such as LDA, LSA, and NMF to contemporary frameworks such as BERTopic and Top2Vec on three Bengali datasets. The experimental results demonstrate the effectiveness of the proposed model by outperforming other models in topic coherence and diversity. In addition, we introduce a novel Bengali dataset called \"NCTBText\" sourced from Bengali textbook materials to enrich and diversify the predominantly newspaper-centric Bengali corpora."
  },
  {
    "url": "http://arxiv.org/abs/2508.00600v1",
    "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large   Language Models",
    "authors": [
      "Mingruo Yuan",
      "Shuyi Zhang",
      "Ben Kao"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines."
  },
  {
    "url": "http://arxiv.org/abs/2508.00589v1",
    "title": "Context-based Motion Retrieval using Open Vocabulary Methods for   Autonomous Driving",
    "authors": [
      "Stefan Englmeier",
      "Max A. B\u00fcttner",
      "Katharina Winter",
      "Fabian B. Flohr"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset."
  },
  {
    "url": "http://arxiv.org/abs/2508.00574v1",
    "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via   Synthetic Continuous Chain-of-Thought",
    "authors": [
      "Jianwei Wang",
      "Ziming Wu",
      "Fuming Lai",
      "Shaobing Lian",
      "Ziqian Zeng"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \\textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off."
  },
  {
    "url": "http://arxiv.org/abs/2508.00555v1",
    "title": "Activation-Guided Local Editing for Jailbreaking Attacks",
    "authors": [
      "Jiecong Wang",
      "Haoran Li",
      "Hao Peng",
      "Ziqian Zeng",
      "Zihao Wang",
      "Haohua Du",
      "Zhengtao Yu"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2508.00554v1",
    "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism",
    "authors": [
      "Li Zhao",
      "Rui Sun",
      "Zuoyou Jiang",
      "Bo Yang",
      "Yuxiao Bai",
      "Mengting Chen",
      "Xinyang Wang",
      "Jing Li",
      "Zuo Bai"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics."
  },
  {
    "url": "http://arxiv.org/abs/2508.00544v1",
    "title": "PaPaformer: Language Model from Pre-trained Paraller Paths",
    "authors": [
      "Joonas Tapaninaho",
      "Mourad Oussala"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:The training of modern large-language models requires an increasingly amount of computation power and time. Even smaller variants, such as small-language models (SLMs), take several days to train in the best-case scenarios, often requiring multiple GPUs. This paper explores methods to train and evaluate decoder-only transformer-based language models in hours instead of days/weeks. We introduces \\textit{PaPaformer}, a decoder-only transformer architecture variant, whose lower-dimensional parallel paths are combined into larger model. The paper shows that these lower-dimensional paths can be trained individually with different types of training data and then combined into one larger model. This method gives the option to reduce the total number of model parameters and the training time with increasing performance. Moreover, the use of parallel path structure opens interesting possibilities to customize paths to accommodate specific task requirements."
  },
  {
    "url": "http://arxiv.org/abs/2508.00537v1",
    "title": "The Prosody of Emojis",
    "authors": [
      "Giulio Zhou",
      "Tsz Kin Lam",
      "Alexandra Birch",
      "Barry Haddow"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Prosodic features such as pitch, timing, and intonation are central to spoken communication, conveying emotion, intent, and discourse structure. In text-based settings, where these cues are absent, emojis act as visual surrogates that add affective and pragmatic nuance. This study examines how emojis influence prosodic realisation in speech and how listeners interpret prosodic cues to recover emoji meanings. Unlike previous work, we directly link prosody and emoji by analysing actual human speech data, collected through structured but open-ended production and perception tasks. This provides empirical evidence of how emoji semantics shape spoken delivery and perception. Results show that speakers adapt their prosody based on emoji cues, listeners can often identify the intended emoji from prosodic variation alone, and greater semantic differences between emojis correspond to increased prosodic divergence. These findings suggest that emojis can act as meaningful carriers of prosodic intent, offering insight into their communicative role in digitally mediated contexts."
  },
  {
    "url": "http://arxiv.org/abs/2508.00534v1",
    "title": "Towards a unified framework for programming paradigms: A systematic   review of classification formalisms and methodological foundations",
    "authors": [
      "Mikel Vandeloise"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:The rise of multi-paradigm languages challenges traditional classification methods, leading to practical software engineering issues like interoperability defects. This systematic literature review (SLR) maps the formal foundations of programming paradigms. Our objective is twofold: (1) to assess the state of the art of classification formalisms and their limitations, and (2) to identify the conceptual primitives and mathematical frameworks for a more powerful, reconstructive approach."
  },
  {
    "url": "http://arxiv.org/abs/2508.00522v1",
    "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in   Fine-Tuning Large Language Models and Beyond",
    "authors": [
      "Jiaxin Deng",
      "Qingcheng Zhu",
      "Junbiao Pang",
      "Linlin Yang",
      "Zhongqian Fu",
      "Baochang Zhang"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Little research explores the correlation between the expressive ability and generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware Minimization (SAM) improves model generalization for both Convolutional Neural Networks (CNNs) and Transformers by encouraging convergence to locally flat minima. However, the connection between sharpness and generalization has not been fully explored for LoRA due to the lack of tools to either empirically seek flat minima or develop theoretical methods. In this work, we propose Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for LoRA. Concretely, we theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace. This approach eliminates the potential interference introduced by perturbations across multiple matrices in the low-rank subspace. Our extensive experiments on large language models and vision-language models demonstrate that EFlat-LoRA achieves optimize efficiency comparable to that of LoRA while simultaneously attaining comparable or even better performance. For example, on the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively. These empirical results also verify that the generalization of LoRA is closely related to sharpness, which is omitted by previous methods."
  },
  {
    "url": "http://arxiv.org/abs/2508.00518v1",
    "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos",
    "authors": [
      "Shuo Liang",
      "Yiwu Zhong",
      "Zi-Yuan Hu",
      "Yeyao Tao",
      "Liwei Wang"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at this https URL ."
  },
  {
    "url": "http://arxiv.org/abs/2508.00489v1",
    "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth   Detection",
    "authors": [
      "Yixuan Tang",
      "Jincheng Wang",
      "Anthony K. H. Tung"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about what is left unsaid. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification."
  },
  {
    "url": "http://arxiv.org/abs/2508.00476v1",
    "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting   Transcripts",
    "authors": [
      "Jeongwoo Kang",
      "Markarit Vartampetian",
      "Felix Herron",
      "Yongxin Zhou",
      "Diandra Fabre",
      "Gabriela Gonzalez-Saez"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:This paper documents GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025. We participated in Task B: question-answering based on meeting transcripts. Our method is based on a retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR). We propose three systems combining these two approaches. Our results show that incorporating AMR leads to high-quality responses for approximately 35% of the questions and provides notable improvements in answering questions that involve distinguishing between different participants (e.g., who questions)."
  },
  {
    "url": "http://arxiv.org/abs/2508.00454v1",
    "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
    "authors": [
      "Yuqi Tang",
      "Kehua Feng",
      "Yunfeng Wang",
      "Zhiwen Chen",
      "Chengfei Lv",
      "Gang Yu",
      "Qiang Zhang",
      "Keyan Ding"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness."
  },
  {
    "url": "http://arxiv.org/abs/2508.00429v1",
    "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network",
    "authors": [
      "Minghao Guo",
      "Xi Zhu",
      "Jingyuan Huang",
      "Kai Mei",
      "Yongfeng Zhang"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning."
  },
  {
    "url": "http://arxiv.org/abs/2508.00420v1",
    "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence   Embedding",
    "authors": [
      "Rana Salama",
      "Abdou Youssef",
      "Mona Diab"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Wavelets have emerged as a cutting edge technology in a number of fields. Concrete results of their application in Image and Signal processing suggest that wavelets can be effectively applied to Natural Language Processing (NLP) tasks that capture a variety of linguistic properties. In this paper, we leverage the power of applying Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We first evaluate, intrinsically and extrinsically, how wavelets can effectively be used to consolidate important information in a word vector while reducing its dimensionality. We further combine DWT with Discrete Cosine Transform (DCT) to propose a non-parameterized model that compresses a sentence with a dense amount of information in a fixed size vector based on locally varying word features. We show the efficacy of the proposed paradigm on downstream applications models yielding comparable and even superior (in some tasks) results to original embeddings."
  },
  {
    "url": "http://arxiv.org/abs/2508.00414v1",
    "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent   Foundation Models Training",
    "authors": [
      "Tianqing Fang",
      "Zhisong Zhang",
      "Xiaoyang Wang",
      "Rui Wang",
      "Can Qin",
      "Yuxuan Wan",
      "Jun-Yu Ma",
      "Ce Zhang",
      "Jiaqi Chen",
      "Xiyun Li",
      "Hongming Zhang",
      "Haitao Mi",
      "Dong Yu"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at this https URL"
  },
  {
    "url": "http://arxiv.org/abs/2508.00408v1",
    "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions",
    "authors": [
      "Dong Huang",
      "Jie M. Zhang",
      "Mark Harman",
      "Qianru Zhang",
      "Mingzhe Du",
      "See-Kiong Ng"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Recently, large language models (LLMs) have shown great promise in automating unit test generation, significantly reducing the manual effort required by developers. To effectively evaluate the capabilities of LLMs in this domain, it is crucial to have a well-designed benchmark that accurately reflects real-world scenarios and mitigates common pitfalls. Existing LLM test generation benchmarks are limited by two critical drawbacks: data contamination and structurally simple function code. As a result, we often cannot rely on the validity of scientific conclusions drawn from empirical studies using these limited benchmarks. The empirical evidence presented may be biased due to contamination and may fail to generalize beyond toy programs due to structural simplicity."
  },
  {
    "url": "http://arxiv.org/abs/2508.00390v1",
    "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV   Vision-Language Navigation",
    "authors": [
      "Hengxing Cai",
      "Jinhan Dong",
      "Yijie Rao",
      "Jingcheng Deng",
      "Jingjun Tan",
      "Qien Chen",
      "Haidong Wang",
      "Zhen Wang",
      "Shiyu Huang",
      "Agachai Sumalee",
      "Renxin Zhong"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable agents to accurately localize targets and plan flight paths in complex environments based on natural language instructions, with broad applications in intelligent inspection, disaster rescue, and urban monitoring. Recent progress in Vision-Language Models (VLMs) has provided strong semantic understanding for this task, while reinforcement learning (RL) has emerged as a promising post-training strategy to further improve generalization. However, existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement. To address these challenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)}, a novel training framework that systematically integrates Curriculum Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator (SA-DE) to quantify the complexity of training samples and a Gaussian Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution, enabling a smooth progression from easy to challenging tasks. This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance. Extensive experiments on the CityNav benchmark demonstrate that SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability. The implementation of our approach is publicly available."
  },
  {
    "url": "http://arxiv.org/abs/2508.00385v1",
    "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness",
    "authors": [
      "Dingzirui Wang",
      "Xuangliang Zhang",
      "Keyan Xu",
      "Qingfu Zhu",
      "Wanxiang Che",
      "Yang Deng"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Numerous studies have investigated the underlying mechanisms of in-context learning (ICL) effectiveness to inspire the design of related methods. However, existing work predominantly assumes the effectiveness of the demonstrations provided within ICL, while many research indicates that not all demonstrations are effective, failing to yielding any performance improvement during ICL. Therefore, in this paper, we investigate the reasons behind demonstration ineffectiveness. Our analysis is based on gradient flow and linear self-attention models. By setting the gradient flow to zero, we deduce that a demonstration becomes ineffective if its information has either been learned by the model or is irrelevant to the user query. Furthermore, we demonstrate that in multi-layer models, the disparity in effectiveness among demonstrations is amplified with layer increasing, causing the model to focus more on effective ones. Considering that current demonstration selection methods primarily focus on the relevance to the user query while overlooking the information that the model has already assimilated, we propose a novel method called GradS, which leverages gradient flow for demonstration selection. We use the magnitude of the gradient flow of the demonstration with respect to a given user query as the criterion, thereby ensuring the effectiveness of the chosen ones. We validate our derivation and GradS on four prominent LLMs across five mainstream datasets. The experimental results confirm that the disparity in effectiveness among demonstrations is magnified as the model layer increases, substantiating our derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on average over the strongest baselines, demonstrating its effectiveness."
  },
  {
    "url": "http://arxiv.org/abs/2508.00370v1",
    "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level   Efficiency for Edge Devices",
    "authors": [
      "Jiyu Chen",
      "Poh Seng Lim",
      "Shuang Peng",
      "Daxiong Luo",
      "JungHau Foo",
      "Yap Deep",
      "Timothy Lee Jun Jie",
      "Kelvin Teh Kae Wen",
      "Fan Yang",
      "Danyu Feng",
      "Hao-Yun Chen",
      "Peng-Wen Chen",
      "Fangyuan Li",
      "Xiaoxin Chen",
      "Wong Wai Mun"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices."
  },
  {
    "url": "http://arxiv.org/abs/2508.00360v1",
    "title": "Lucy: edgerunning agentic web search on mobile with machine generated   task vectors",
    "authors": [
      "Alan Dao",
      "Dinh Bach Vu",
      "Alex Nguyen",
      "Norapat Buppodom"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Small language models (SLMs) are inherently limited in knowledge-intensive tasks due to their constrained capacity. While test-time computation offers a path to enhanced performance, most approaches treat reasoning as a fixed or heuristic process. In this work, we propose a new paradigm: viewing the model's internal reasoning, delimited by <think> and </think> tags, as a dynamic task vector machine. Rather than treating the content inside these tags as a mere trace of thought, we interpret the generation process itself as a mechanism through which the model \\textbf{constructs and refines its own task vectors} on the fly. We developed a method to optimize this dynamic task vector machine through RLVR and successfully trained an agentic web-search model. We present Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models such as DeepSeek-V3. This demonstrates that small models can rival large ones when equipped with structured, self-constructed task reasoning."
  },
  {
    "url": "http://arxiv.org/abs/2508.00344v1",
    "title": "PilotRL: Training Language Model Agents via Global Planning-Guided   Progressive Reinforcement Learning",
    "authors": [
      "Keer Lu",
      "Chong Chen",
      "Bin Cui",
      "Huang Leng",
      "Wentao Zhang"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale."
  },
  {
    "url": "http://arxiv.org/abs/2508.00332v1",
    "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with   Object-Phrase Alignment",
    "authors": [
      "Kaiyan Zhao",
      "Zhongtao Miao",
      "Yoshimasa Tsuruoka"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Multimodal sentence embedding models typically leverage image-caption pairs in addition to textual data during training. However, such pairs often contain noise, including redundant or irrelevant information on either the image or caption side. To mitigate this issue, we propose MCSEO, a method that enhances multimodal sentence embeddings by incorporating fine-grained object-phrase alignment alongside traditional image-caption alignment. Specifically, MCSEO utilizes existing segmentation and object detection models to extract accurate object-phrase pairs, which are then used to optimize a contrastive learning objective tailored to object-phrase correspondence. Experimental results on semantic textual similarity (STS) tasks across different backbone models demonstrate that MCSEO consistently outperforms strong baselines, highlighting the significance of precise object-phrase alignment in multimodal representation learning."
  },
  {
    "url": "http://arxiv.org/abs/2508.00324v1",
    "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety   Knowledge",
    "authors": [
      "Yeonjun In",
      "Wonjoong Kim",
      "Sangwu Park",
      "Chanyoung Park"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Although large reasoning models (LRMs) have demonstrated impressive capabilities on complex tasks, recent studies reveal that these models frequently fulfill harmful user instructions, raising significant safety concerns. In this paper, we investigate the underlying cause of LRM safety risks and find that models already possess sufficient safety knowledge but fail to activate it during reasoning. Based on this insight, we propose R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process. R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach."
  },
  {
    "url": "http://arxiv.org/abs/2508.00305v1",
    "title": "Systematic Evaluation of Optimization Techniques for Long-Context   Language Models",
    "authors": [
      "Ammar Ahmed",
      "Sheng Di",
      "Franck Cappello",
      "Zirui Liu",
      "Jingoo Han",
      "Ali Anwar"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like pruning, quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two LLM architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps LLM practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations."
  },
  {
    "url": "http://arxiv.org/abs/2508.00285v1",
    "title": "Integrating clinical reasoning into large language model-based diagnosis   through etiology-aware attention steering",
    "authors": [
      "Peixian Li",
      "Yu Tian",
      "Ruiqi Tu",
      "Chengkai Wu",
      "Jingjing Ren",
      "Jingsong Li"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Objective: Large Language Models (LLMs) demonstrate significant capabilities in medical text understanding and generation. However, their diagnostic reliability in complex clinical scenarios remains limited. This study aims to enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We propose an Etiology-Aware Attention Steering Framework to integrate structured clinical reasoning into LLM-based diagnosis. Specifically, we first construct Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines for three representative acute abdominal emergencies: acute appendicitis, acute pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head Identification algorithm to pinpoint attention heads crucial for the model's etiology reasoning. To ensure reliable clinical reasoning alignment, we introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds etiological reasoning cues into input representations and steers the selected Etiology-Aware Heads toward critical information through a Reasoning-Guided Loss function. Result: On the Consistent Diagnosis Cohort, our framework improves average diagnostic accuracy by 15.65% and boosts the average Reasoning Focus Score by 31.6% over baselines. External validation on the Discrepant Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic accuracy. Further assessments via Reasoning Attention Frequency indicate that our models exhibit enhanced reliability when faced with real-world complex scenarios. Conclusion: This study presents a practical and effective approach to enhance clinical reasoning in LLM-based diagnosis. By aligning model attention with structured CRS, the proposed framework offers a promising paradigm for building more interpretable and reliable AI diagnostic systems in complex clinical settings."
  },
  {
    "url": "http://arxiv.org/abs/2508.00282v1",
    "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks",
    "authors": [
      "Yi-Long Lu",
      "Jiajun Song",
      "Chunhui Zhang",
      "Wei Wang"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied this http URL conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents."
  },
  {
    "url": "http://arxiv.org/abs/2508.00271v1",
    "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \\textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2508.00238v1",
    "title": "Model Misalignment and Language Change: Traces of AI-Associated Language   in Unscripted Spoken English",
    "authors": [
      "Bryce Anderson",
      "Riley Galpin",
      "Tom S. Juzek"
    ],
    "date": "2025-08-01",
    "abstract": "View PDF HTML (experimental)Abstract:In recent years, written language, particularly in science and education, has undergone remarkable shifts in word usage. These changes are widely attributed to the growing influence of Large Language Models (LLMs), which frequently rely on a distinct lexical style. Divergences between model output and target audience norms can be viewed as a form of misalignment. While these shifts are often linked to using Artificial Intelligence (AI) directly as a tool to generate text, it remains unclear whether the changes reflect broader changes in the human language system itself. To explore this question, we constructed a dataset of 22.1 million words from unscripted spoken language drawn from conversational science and technology podcasts. We analyzed lexical trends before and after ChatGPT's release in 2022, focusing on commonly LLM-associated words. Our results show a moderate yet significant increase in the usage of these words post-2022, suggesting a convergence between human word choices and LLM-associated patterns. In contrast, baseline synonym words exhibit no significant directional shift. Given the short time frame and the number of words affected, this may indicate the onset of a remarkable shift in language use. Whether this represents natural language change or a novel shift driven by AI exposure remains an open question. Similarly, although the shifts may stem from broader adoption patterns, it may also be that upstream training misalignments ultimately contribute to changes in human language use. These findings parallel ethical concerns that misaligned models may shape social and moral beliefs."
  },
  {
    "url": "http://arxiv.org/abs/2508.00230v1",
    "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using   Khatri--Rao Product",
    "authors": [
      "Paul Albert",
      "Frederic Z. Zhang",
      "Hemanth Saratchandran",
      "Anton van den Hengel",
      "Ehsan Abbasnejad"
    ],
    "date": "2025-08-01",
    "abstract": "View PDFAbstract:Parameter-efficient fine-tuning (PEFT) has become a standard approach for adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation (LoRA) has achieved notable success. However, recent studies have highlighted its limitations compared against full-rank alternatives, particularly when applied to multimodal and large language models. In this work, we present a quantitative comparison amongst full-rank and low-rank PEFT methods using a synthetic matrix approximation benchmark with controlled spectral properties. Our results confirm that LoRA struggles to approximate matrices with relatively flat spectrums or high frequency components -- signs of high effective ranks. To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the Khatri-Rao product to produce weight updates, which, by construction, tends to produce matrix product with a high effective rank. We demonstrate performance gains with KRAdapter on vision-language models up to 1B parameters and on large language models up to 8B parameters, particularly on unseen common-sense reasoning tasks. In addition, KRAdapter maintains the memory and compute efficiency of LoRA, making it a practical and robust alternative to fine-tune billion-scale parameter models."
  },
  {
    "url": "http://arxiv.org/abs/2508.00222v1",
    "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in   Reinforcement Learning with Hybrid-policy Optimization",
    "authors": [
      "Yihong Dong",
      "Xue Jiang",
      "Yongding Tao",
      "Huanyu Liu",
      "Kechi Zhang",
      "Lili Mou",
      "Rongyu Cao",
      "Yingwei Ma",
      "Jue Chen",
      "Binhua Li",
      "Zhi Jin",
      "Fei Huang",
      "Yongbin Li",
      "Ge Li"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its inherently on-policy strategy with LLM's immense action space and sparse reward. Further, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel approach that synergizes internal exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components: Multiple Importance Sampling to address for distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. The results show that RL-PLUS achieves state-of-the-art performance compared with existing RLVR methods on six math reasoning benchmarks and exhibits superior performance on six out-of-distribution reasoning tasks. It also achieves consistent and significant gains across diverse model families, with average relative improvements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across multiple benchmarks indicate that RL-PLUS effectively resolves the capability boundary collapse problem."
  },
  {
    "url": "http://arxiv.org/abs/2508.00220v1",
    "title": "Semantic Compression for Word and Sentence Embeddings using Discrete   Wavelet Transform",
    "authors": [
      "Rana Aref Salama",
      "Abdou Youssef",
      "Mona Diab"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Wavelet transforms, a powerful mathematical tool, have been widely used in different domains, including Signal and Image processing, to unravel intricate patterns, enhance data representation, and extract meaningful features from data. Tangible results from their application suggest that Wavelet transforms can be applied to NLP capturing a variety of linguistic and semantic properties. In this paper, we empirically leverage the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase the capabilities of DWT in analyzing embedding representations at different levels of resolution and compressing them while maintaining their overall quality. We assess the effectiveness of DWT embeddings on semantic similarity tasks to show how DWT can be used to consolidate important semantic information in an embedding vector. We show the efficacy of the proposed paradigm using different embedding models, including large language models, on downstream tasks. Our results show that DWT can reduce the dimensionality of embeddings by 50-93% with almost no change in performance for semantic similarity tasks, while achieving superior accuracy in most downstream tasks. Our findings pave the way for applying DWT to improve NLP applications."
  },
  {
    "url": "http://arxiv.org/abs/2508.00217v1",
    "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and   Challenges",
    "authors": [
      "Xiaofeng Wu",
      "Alan Ritter",
      "Wei Xu"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Tables have gained significant attention in large language models (LLMs) and multimodal large language models (MLLMs) due to their complex and flexible structure. Unlike linear text inputs, tables are two-dimensional, encompassing formats that range from well-structured database tables to complex, multi-layered spreadsheets, each with different purposes. This diversity in format and purpose has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging. To address these challenges, this paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. We highlight several critical gaps in the field that indicate the need for further research: (1) the predominance of retrieval-focused tasks that require minimal reasoning beyond mathematical and logical operations; (2) significant challenges faced by models when processing complex table structures, large-scale tables, length context, or multi-table scenarios; and (3) the limited generalization of models across different tabular representations and formats."
  },
  {
    "url": "http://arxiv.org/abs/2508.00185v1",
    "title": "Comparison of Large Language Models for Deployment Requirements",
    "authors": [
      "Alper Yaman",
      "Jannik Schwab",
      "Christof Nitsche",
      "Abhirup Sinha",
      "Marco Huber"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs), such as Generative Pre-trained Transformers (GPTs) are revolutionizing the generation of human-like text, producing contextually relevant and syntactically correct content. Despite challenges like biases and hallucinations, these Artificial Intelligence (AI) models excel in tasks, such as content creation, translation, and code generation. Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address these issues. Over the past two years, numerous open-source foundational and fine-tuned models have been introduced, complicating the selection of the optimal LLM for researchers and companies regarding licensing and hardware requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM selection, we present a comparative list of foundational and domain-specific models, focusing on features, such as release year, licensing, and hardware requirements. This list is published on GitLab and will be continuously updated."
  },
  {
    "url": "http://arxiv.org/abs/2508.00171v1",
    "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in   Multimodal Clinical AI",
    "authors": [
      "David Restrepo",
      "Ira Ktena",
      "Maria Vakalopoulou",
      "Stergios Christodoulidis",
      "Enzo Ferrante"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Clinical decision-making relies on the integrated analysis of medical images and the associated clinical reports. While Vision-Language Models (VLMs) can offer a unified framework for such tasks, they can exhibit strong biases toward one modality, frequently overlooking critical visual cues in favor of textual information. In this work, we introduce Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality in binary classification tasks. By systematically swapping images or text between samples with opposing labels, we expose modality-specific biases. We assess six open-source VLMs-four generalist models and two fine-tuned for medical data-on two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray) and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance and the calibration of every model in both unperturbed and perturbed settings, we reveal a marked dependency on text input, which persists despite the presence of complementary visual information. We also perform a qualitative attention-based analysis which further confirms that image content is often overshadowed by text details. Our findings highlight the importance of designing and evaluating multimodal medical models that genuinely integrate visual and textual cues, rather than relying on single-modality signals."
  },
  {
    "url": "http://arxiv.org/abs/2508.00161v1",
    "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned   LLMs",
    "authors": [
      "Ziqian Zhong",
      "Aditi Raghunathan"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution."
  },
  {
    "url": "http://arxiv.org/abs/2508.00121v1",
    "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?",
    "authors": [
      "Xiao Zhang",
      "Johan bos"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Neural semantic parsers have shown good overall performance for a variety of linguistic phenomena, reaching semantic matching scores of more than 90%. But how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation? A case in point is English verb phrase ellipsis, a construct where entire verb phrases can be abbreviated by a single auxiliary verb. Are the otherwise known as powerful semantic parsers able to deal with ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers. Although these parsers performed very well on the standard test set, they failed in the instances with ellipsis. Data augmentation"
  },
  {
    "url": "http://arxiv.org/abs/2508.00109v1",
    "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form   Factuality",
    "authors": [
      "Mingda Chen",
      "Yang Li",
      "Xilun Chen",
      "Adina Williams",
      "Gargi Ghosh",
      "Scott Yih"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts."
  },
  {
    "url": "http://arxiv.org/abs/2508.00095v1",
    "title": "Semiotic Complexity and Its Epistemological Implications for Modeling   Culture",
    "authors": [
      "Zachary K. Stine",
      "James E. Deitrick"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Greater theorizing of methods in the computational humanities is needed for epistemological and interpretive clarity, and therefore the maturation of the field. In this paper, we frame such modeling work as engaging in translation work from a cultural, linguistic domain into a computational, mathematical domain, and back again. Translators benefit from articulating the theory of their translation process, and so do computational humanists in their work -- to ensure internal consistency, avoid subtle yet consequential translation errors, and facilitate interpretive transparency. Our contribution in this paper is to lay out a particularly consequential dimension of the lack of theorizing and the sorts of translation errors that emerge in our modeling practices as a result. Along these lines we introduce the idea of semiotic complexity as the degree to which the meaning of some text may vary across interpretive lenses, and make the case that dominant modeling practices -- especially around evaluation -- commit a translation error by treating semiotically complex data as semiotically simple when it seems epistemologically convenient by conferring superficial clarity. We then lay out several recommendations for researchers to better account for these epistemological issues in their own work."
  },
  {
    "url": "http://arxiv.org/abs/2508.00086v1",
    "title": "Do LLMs produce texts with \"human-like\" lexical diversity?",
    "authors": [
      "Kelly Kendro",
      "Jeffrey Maloney",
      "Scott Jarvis"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:The degree to which LLMs produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini, and -4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and Support Vector Machines revealed that the LLM-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that LLMs do not produce human-like texts in relation to lexical diversity, and the newer LLMs produce less human-like texts than older models. We discuss the implications of these results for language pedagogy and related applications."
  },
  {
    "url": "http://arxiv.org/abs/2508.00083v1",
    "title": "A Survey on Code Generation with LLM-based Agents",
    "authors": [
      "Yihong Dong",
      "Xue Jiang",
      "Jiaru Qian",
      "Tian Wang",
      "Kechi Zhang",
      "Zhi Jin",
      "Ge Li"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Code generation agents powered by large language models (LLMs) are revolutionizing the software development paradigm. Distinct from previous code generation techniques, code generation agents are characterized by three core features. 1) Autonomy: the ability to independently manage the entire workflow, from task decomposition to coding and debugging. 2) Expanded task scope: capabilities that extend beyond generating code snippets to encompass the full software development lifecycle (SDLC). 3) Enhancement of engineering practicality: a shift in research emphasis from algorithmic innovation toward practical engineering challenges, such as system reliability, process management, and tool integration. This domain has recently witnessed rapid development and an explosion in research, demonstrating significant application potential. This paper presents a systematic survey of the field of LLM-based code generation agents. We trace the technology's developmental trajectory from its inception and systematically categorize its core techniques, including both single-agent and multi-agent architectures. Furthermore, this survey details the applications of LLM-based agents across the full SDLC, summarizes mainstream evaluation benchmarks and metrics, and catalogs representative tools. Finally, by analyzing the primary challenges, we identify and propose several foundational, long-term research directions for the future work of the field."
  },
  {
    "url": "http://arxiv.org/abs/2508.00079v1",
    "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning   Proficiency of Large Language Models on Physics Problems",
    "authors": [
      "Oshayer Siddique",
      "J. M Areeb Uzair Alam",
      "Md Jobayer Rahman Rafy",
      "Syed Rifat Raiyan",
      "Hasan Mahmud",
      "Md Kamrul Hasan"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23776v1",
    "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem   Solving Capabilities",
    "authors": [
      "Yunxiang Yan",
      "Tomohiro Sawada",
      "Kartik Goyal"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:While question-answering~(QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on \\emph{cascaded question disclosure} that provides a more accurate estimate of the models' problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models. We further validate our findings by extensive ablation studies."
  },
  {
    "url": "http://arxiv.org/abs/2507.23773v1",
    "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning   Architecture with LLM-Based World Model",
    "authors": [
      "Mingkai Deng",
      "Jinyu Hou",
      "Yilin Shen",
      "Hongxia Jin",
      "Graham Neubig",
      "Zhiting Hu",
      "Eric Xing"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \\modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \\modelname improves the success of flight search from 0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent advantage of up to 124\\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \\modelname with pretrained LLMs, available as a research demo for public testing."
  },
  {
    "url": "http://arxiv.org/abs/2507.23751v1",
    "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning   and non-reasoning tasks",
    "authors": [
      "Ping Yu",
      "Jack Lanchantin",
      "Tianlu Wang",
      "Weizhe Yuan",
      "Olga Golovneva",
      "Ilia Kulikov",
      "Sainbayar Sukhbaatar",
      "Jason Weston",
      "Jing Xu"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard."
  },
  {
    "url": "http://arxiv.org/abs/2507.23740v1",
    "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs",
    "authors": [
      "Nasim Shirvani-Mahdavi",
      "Devin Wingfield",
      "Amin Ghasemi",
      "Chengkai Li"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at this https URL}{this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23726v2",
    "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
    "authors": [
      "Luoxin Chen",
      "Jinming Gu",
      "Liankai Huang",
      "Wenhao Huang",
      "Zhicheng Jiang",
      "Allan Jie",
      "Xiaoran Jin",
      "Xing Jin",
      "Chenggang Li",
      "Kaijing Ma",
      "Cheng Ren",
      "Jiawei Shen",
      "Wenlei Shi",
      "Tong Sun",
      "He Sun",
      "Jiahui Wang",
      "Siran Wang",
      "Zhihong Wang",
      "Chenrui Wei",
      "Shufa Wei",
      "Yonghui Wu",
      "Yuchen Wu",
      "Yihang Xia",
      "Huajian Xin",
      "Fan Yang",
      "Huaiyuan Ying",
      "Hongyi Yuan",
      "Zheng Yuan",
      "Tianyang Zhan",
      "Chi Zhang",
      "Yue Zhang",
      "Ge Zhang",
      "Tianyun Zhao",
      "Jianqiu Zhao",
      "Yichi Zhou",
      "Thomas Hanwen Zhu"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning."
  },
  {
    "url": "http://arxiv.org/abs/2507.23701v1",
    "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
    "authors": [
      "Long Phan",
      "Mantas Mazeika",
      "Andy Zou",
      "Dan Hendrycks"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23674v1",
    "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached   Responses",
    "authors": [
      "Muhammad Taha Cheema",
      "Abeer Aamir",
      "Khawaja Gul Muhammad",
      "Naveed Anwar Bhatti",
      "Ihsan Ayyub Qazi",
      "Zafar Ayyub Qazi"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience."
  },
  {
    "url": "http://arxiv.org/abs/2507.23661v1",
    "title": "Arabic Hate Speech Identification and Masking in Social Media using Deep   Learning Models and Pre-trained Models Fine-tuning",
    "authors": [
      "Salam Thabet Doghmash",
      "Motaz Saad"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Hate speech identification in social media has become an increasingly important issue in recent years. In this research, we address two problems: 1) to detect hate speech in Arabic text, 2) to clean a given text from hate speech. The meaning of cleaning here is replacing each bad word with stars based on the number of letters for each word. Regarding the first problem, we conduct several experiments using deep learning models and transformers to determine the best model in terms of the F1 score. Regarding second problem, we consider it as a machine translation task, where the input is a sentence containing dirty text and the output is the same sentence with masking the dirty text. The presented methods achieve the best model in hate speech detection with a 92\\% Macro F1 score and 95\\% accuracy. Regarding the text cleaning experiment, the best result in the hate speech masking model reached 0.3 in BLEU score with 1-gram, which is a good result compared with the state of the art machine translation systems."
  },
  {
    "url": "http://arxiv.org/abs/2507.23607v1",
    "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with   Uncertainty Estimates",
    "authors": [
      "Tien Huu Do",
      "Antoine Masquelier",
      "Nae Eoun Lee",
      "Jonathan Crowther"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models."
  },
  {
    "url": "http://arxiv.org/abs/2507.23588v1",
    "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models",
    "authors": [
      "Alexandre Misrahi",
      "Nadezhda Chirkova",
      "Maxime Louis",
      "Vassilina Nikoulina"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Differential Transformer has recently been proposed to improve performance in Transformer models by canceling out noise through a denoiser attention mechanism. In this work, we introduce DiffLoRA, a parameter-efficient adaptation of the differential attention mechanism, with low-rank adapters on both positive and negative attention terms. This approach retains the efficiency of LoRA while aiming to benefit from the performance gains of differential attention. We evaluate DiffLoRA across a broad range of NLP tasks, including general benchmarks, many-shot in-context learning, RAG, and long-context tests. We observe that, although DiffLoRA falls short of other parameter-efficient fine-tuning methods in most evaluation tasks, it shows interesting results in certain domains (+11 pts on LoRA for HumanEval). We analyze the attention patterns post-finetuning to identify the reasons for this behavior."
  },
  {
    "url": "http://arxiv.org/abs/2507.23577v1",
    "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of   Adversarial Machine-Generated Text",
    "authors": [
      "Alva West",
      "Luodan Zhang",
      "Liuliu Zhang",
      "Minjun Zhu",
      "Yixuan Weng",
      "Yue Zhang"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:The proliferation of sophisticated text generation models necessitates the development of robust detection methods capable of identifying machine-generated content, particularly text designed to evade detection through adversarial perturbations. Existing zero-shot detectors often rely on statistical measures that implicitly assume Gaussian distributions, a premise that falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. This paper introduces T-Detect, a novel detection method that fundamentally redesigns the statistical core of curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23541v1",
    "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning",
    "authors": [
      "Keer Lu",
      "Zheng Liang",
      "Youquan Li",
      "Jiejun Tan",
      "Da Pan",
      "Shusen Zhang",
      "Guosheng Dong",
      "Huang Leng"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented **R**easoning framework driven by progressive **R**einforcement learning. In this framework, we first develop the model's ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the model's retrieval and reasoning coordination. Extensive experiments indicate that **Med-R$^3$** could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53\\%."
  },
  {
    "url": "http://arxiv.org/abs/2507.23511v1",
    "title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio   Understanding Tasks",
    "authors": [
      "Yadong Niu",
      "Tianzi Wang",
      "Heinrich Dinkel",
      "Xingwei Sun",
      "Jiahao Zhou",
      "Gang Li",
      "Jizhong Liu",
      "Xunying Liu",
      "Junbo Zhang",
      "Jian Luan"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:While large audio-language models have advanced open-ended audio understanding, they still fall short of nuanced human-level comprehension. This gap persists largely because current benchmarks, limited by data annotations and evaluation metrics, fail to reliably distinguish between generic and highly detailed model outputs. To this end, this work introduces MECAT, a Multi-Expert Constructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via a pipeline that integrates analysis from specialized expert models with Chain-of-Thought large language model reasoning, MECAT provides multi-perspective, fine-grained captions and open-set question-answering pairs. The benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced Audio Text Evaluation). This metric penalizes generic terms and rewards detailed descriptions by combining single-sample semantic similarity with cross-sample discriminability. A comprehensive evaluation of state-of-the-art audio models is also presented, providing new insights into their current capabilities and limitations. The data and code are available at this https URL"
  },
  {
    "url": "http://arxiv.org/abs/2507.23486v1",
    "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and   Effectiveness in Clinical Domains",
    "authors": [
      "Shirui Wang",
      "Zhihui Tang",
      "Huaxia Yang",
      "Qiuhong Gong",
      "Tiantian Gu",
      "Hongyang Ma",
      "Yongxin Wang",
      "Wubin Sun",
      "Zeliang Lian",
      "Kehang Mao",
      "Yinan Jiang",
      "Zhicheng Huang",
      "Lingyun Ma",
      "Wenjie Shen",
      "Yajie Ji",
      "Yunhui Tan",
      "Chunbo Wang",
      "Yunlu Gao",
      "Qianling Ye",
      "Rui Lin",
      "Mingyu Chen",
      "Lijuan Niu",
      "Zhihao Wang",
      "Peng Yu",
      "Mengran Lang",
      "Yue Liu",
      "Huimin Zhang",
      "Haitao Shen",
      "Long Chen",
      "Qiguang Zhao",
      "Si-Xuan Liu",
      "Lina Zhou",
      "Hua Gao",
      "Dongqiang Ye",
      "Lingmin Meng",
      "Youtao Yu",
      "Naixin Liang",
      "Jianxiong Wu"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%), with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments."
  },
  {
    "url": "http://arxiv.org/abs/2507.23465v1",
    "title": "Role-Aware Language Models for Secure and Contextualized Access Control   in Organizations",
    "authors": [
      "Saeed Almheiri",
      "Yerulan Kongrat",
      "Adrian Santosh",
      "Ruslan Tasmukhanov",
      "Josemaria Vera",
      "Muhammad Dehan Al Kautsar",
      "Fajri Koto"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts."
  },
  {
    "url": "http://arxiv.org/abs/2507.23453v1",
    "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based   Evaluation Systems",
    "authors": [
      "Lijia Liu",
      "Takumi Kondo",
      "Kyohei Atarashi",
      "Koh Takeuchi",
      "Jiyi Li",
      "Shigeru Saito",
      "Hisashi Kashima"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:This paper investigates defenses for LLM-based evaluation systems against prompt injection. We formalize a class of threats called blind attacks, where a candidate answer is crafted independently of the true answer to deceive the evaluator. To counter such attacks, we propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates the submission against a deliberately false ground-truth answer. An attack is detected if the system validates an answer under both standard and counterfactual conditions. Experiments show that while standard evaluation is highly vulnerable, our SE+CFE framework significantly improves security by boosting attack detection with minimal performance trade-offs."
  },
  {
    "url": "http://arxiv.org/abs/2507.23407v1",
    "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to   Enhance Human-AI Collaboration",
    "authors": [
      "Ante Wang",
      "Yujie Lin",
      "Jingyao Liu",
      "Suhang Wu",
      "Hao Liu",
      "Xinyan Xiao",
      "Jinsong Su"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Critical thinking is essential for building robust AI systems, preventing them from blindly accepting flawed data or biased reasoning. However, prior work has primarily focused on passive critical thinking, where models simply reject problematic queries without taking constructive steps to address user requests. In this work, we introduce proactive critical thinking, a paradigm where models actively seek missing or clarifying information from users to resolve their queries better. To evaluate this capability, we present GSM-MC and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math problems with a key variable deliberately removed, requiring models to identify and request the missing information. GSM-MCE further increases the difficulty by introducing irrelevant details to test robustness against distractions. Experiments on Qwen3 and Llama series models show that, while these models excel in traditional reasoning tasks due to extensive post-training and inference-time scaling, they struggle with proactive critical thinking, especially smaller ones. However, we demonstrate that reinforcement learning (RL) can significantly improve this ability. Using our enhanced RL algorithm, we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to 73.98% on GSM-MC. We hope this work advances models that collaborate more effectively with users in problem-solving through proactive critical thinking."
  },
  {
    "url": "http://arxiv.org/abs/2507.23404v1",
    "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring",
    "authors": [
      "Salah Eddine Bekhouche",
      "Azeddine Benlamoudi",
      "Yazid Bounab",
      "Fadi Dornaika",
      "Abdenour Hadid"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at \\href{this https URL}{GitHub}."
  },
  {
    "url": "http://arxiv.org/abs/2507.23400v1",
    "title": "MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based   on Multi-Relational Graphs and Structural Entropy Minimization",
    "authors": [
      "Yongbing Zhang",
      "Fang Nan",
      "Shengxiang Gao",
      "Yuxin Huang",
      "Kaiwen Tan",
      "Zhengtao Yu"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:The core challenge faced by multi-document summarization is the complexity of relationships among documents and the presence of information redundancy. Graph clustering is an effective paradigm for addressing this issue, as it models the complex relationships among documents using graph structures and reduces information redundancy through clustering, achieving significant research progress. However, existing methods often only consider single-relational graphs and require a predefined number of clusters, which hinders their ability to fully represent rich relational information and adaptively partition sentence groups to reduce redundancy. To overcome these limitations, we propose MRGSEM-Sum, an unsupervised multi-document summarization framework based on multi-relational graphs and structural entropy minimization. Specifically, we construct a multi-relational graph that integrates semantic and discourse relations between sentences, comprehensively modeling the intricate and dynamic connections among sentences across documents. We then apply a two-dimensional structural entropy minimization algorithm for clustering, automatically determining the optimal number of clusters and effectively organizing sentences into coherent groups. Finally, we introduce a position-aware compression mechanism to distill each cluster, generating concise and informative summaries. Extensive experiments on four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently outperforms previous unsupervised methods and, in several cases, achieves performance comparable to supervised models and large language models. Human evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high consistency and coverage, approaching human-level quality."
  },
  {
    "url": "http://arxiv.org/abs/2507.23399v1",
    "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM   Deployment for Translators",
    "authors": [
      "Peter Sandrini"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:The rapid proliferation of Large Language Models presents both opportunities and challenges for the translation field. While commercial, cloud-based AI chatbots have garnered significant attention in translation studies, concerns regarding data privacy, security, and equitable access necessitate exploration of alternative deployment models. This paper investigates the feasibility and performance of locally deployable, free language models as a viable alternative to proprietary, cloud-based AI solutions. This study evaluates three open-source models installed on CPU-based platforms and compared against commercially available online chat-bots. The evaluation focuses on functional performance rather than a comparative analysis of human-machine translation quality, an area already subject to extensive research. The platforms assessed were chosen for their accessibility and ease of use across various operating systems. While local deployment introduces its own challenges, the benefits of enhanced data control, improved privacy, and reduced dependency on cloud services are compelling. The findings of this study contribute to a growing body of knowledge concerning the democratization of AI technology and inform future research and development efforts aimed at making LLMs more accessible and practical for a wider range of users, specifically focusing on the needs of individual translators and small businesses."
  },
  {
    "url": "http://arxiv.org/abs/2507.23386v1",
    "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models",
    "authors": [
      "Ailiang Lin",
      "Zhuoyun Li",
      "Kotaro Funakoshi"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods."
  },
  {
    "url": "http://arxiv.org/abs/2507.23382v1",
    "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints   in Multimodal Large Language Models",
    "authors": [
      "Yiyan Ji",
      "Haoran Chen",
      "Qiguang Chen",
      "Chengyue Wu",
      "Libo Qin",
      "Wanxiang Che"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Multimodal planning capabilities refer to the ability to predict, reason, and design steps for task execution with multimodal context, which is essential for complex reasoning and decision-making across multiple steps. However, current benchmarks face two key challenges: (1) they cannot directly assess multimodal real-world planning capabilities, and (2) they lack constraints or implicit constraints across modalities. To address these issues, we introduce Multimodal Planning with Complex Constraints (MPCC), the first benchmark to systematically evaluate MLLMs' ability to handle multimodal constraints in planning. To address the first challenge, MPCC focuses on three real-world tasks: Flight Planning, Calendar Planning, and Meeting Planning. To solve the second challenge, we introduce complex constraints (e.g. budget, temporal, and spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to separate constraint complexity from search space expansion. Experiments on 13 advanced MLLMs reveal significant challenges: closed-source models achieve only 21.3% feasible plans, while open-source models average below 11%. Additionally, we observe that MLLMs are highly sensitive to constraint complexity and that traditional multimodal prompting strategies fail in multi-constraint scenarios. Our work formalizes multimodal constraints in planning, provides a rigorous evaluation framework, and highlights the need for advancements in constraint-aware reasoning for real-world MLLM applications."
  },
  {
    "url": "http://arxiv.org/abs/2507.23364v1",
    "title": "Holistic Evaluations of Topic Models",
    "authors": [
      "Thomas Compton"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Topic models are gaining increasing commercial and academic interest for their ability to summarize large volumes of unstructured text. As unsupervised machine learning methods, they enable researchers to explore data and help general users understand key themes in large text collections. However, they risk becoming a 'black box', where users input data and accept the output as an accurate summary without scrutiny. This article evaluates topic models from a database perspective, drawing insights from 1140 BERTopic model runs. The goal is to identify trade-offs in optimizing model parameters and to reflect on what these findings mean for the interpretation and responsible use of topic models"
  },
  {
    "url": "http://arxiv.org/abs/2507.23361v1",
    "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
    "authors": [
      "Silin Chen",
      "Shaoxin Lin",
      "Xiaodong Gu",
      "Yuling Shi",
      "Heng Lian",
      "Longfei Yun",
      "Dong Chen",
      "Weiguo Sun",
      "Lin Cao",
      "Qianxiang Wang"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution."
  },
  {
    "url": "http://arxiv.org/abs/2507.23358v1",
    "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction",
    "authors": [
      "Renato Vukovic",
      "Carel van Niekerk",
      "Michael Heck",
      "Benjamin Ruppik",
      "Hsien-Chin Lin",
      "Shutong Feng",
      "Nurul Lubis",
      "Milica Gasic"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability."
  },
  {
    "url": "http://arxiv.org/abs/2507.23348v1",
    "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
    "authors": [
      "Han Li",
      "Yuling Shi",
      "Shaoxin Lin",
      "Xiaodong Gu",
      "Heng Lian",
      "Xin Wang",
      "Yantao Jia",
      "Tao Huang",
      "Qianxiang Wang"
    ],
    "date": "2025-07-31",
    "abstract": "View PDFAbstract:Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin."
  },
  {
    "url": "http://arxiv.org/abs/2507.23336v1",
    "title": "DSBC : Data Science task Benchmarking with Context engineering",
    "authors": [
      "Ram Mohan Rao Kadiyala",
      "Siddhant Gupta",
      "Jebish Purbey",
      "Giulio Martini",
      "Suman Debnath",
      "Hamza Farooq"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents."
  },
  {
    "url": "http://arxiv.org/abs/2507.23334v1",
    "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented   Generation",
    "authors": [
      "Daeyong Kwon",
      "SeungHeon Doh",
      "Juhan Nam"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency."
  },
  {
    "url": "http://arxiv.org/abs/2507.23319v1",
    "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward   Sensitive Content",
    "authors": [
      "Alfio Ferrara",
      "Sergio Picascia",
      "Laura Pinnavaia",
      "Vojimir Ranitovic",
      "Elisabetta Rocchetti",
      "Alice Tuveri"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions. This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts. Our experiments indicate that GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods."
  },
  {
    "url": "http://arxiv.org/abs/2507.23292v1",
    "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made   Easy",
    "authors": [
      "RJ Skerry-Ryan",
      "Julian Salazar",
      "Soroosh Mariooryad",
      "David Kao",
      "Daisy Stanton",
      "Eric Battenberg",
      "Matt Shannon",
      "Ron J. Weiss",
      "Robin Scheibler",
      "Jonas Rothfuss",
      "Tom Bagby"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23279v1",
    "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models",
    "authors": [
      "Zunhai Su",
      "Qingyuan Li",
      "Hao Zhang",
      "YuLei Qian",
      "Yuchen Xie",
      "Kehong Yuan"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23248v1",
    "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark   Creation and Performance Analysis",
    "authors": [
      "Shimanto Bhowmik",
      "Tawsif Tashwar Dipto",
      "Md Sazzad Islam",
      "Sheryl Hsu",
      "Tahsin Reasat"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \\& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23247v1",
    "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication",
    "authors": [
      "Sneha Oram",
      "Pushpak Bhattacharyya"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs."
  },
  {
    "url": "http://arxiv.org/abs/2507.23242v1",
    "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter   with Unstructured Real-World Documents",
    "authors": [
      "Sungguk Cha",
      "DongWook Kim",
      "Taeseung Hahn",
      "Mintae Kim",
      "Youngsub Han",
      "Byoung-Ki Jeon"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3 for multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QR's potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts."
  },
  {
    "url": "http://arxiv.org/abs/2507.23227v1",
    "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker   Data with LLMs",
    "authors": [
      "Sophie Kearney",
      "Shu Yang",
      "Zixuan Wen",
      "Bojian Hou",
      "Duy Duong-Tran",
      "Tianlong Chen",
      "Jason Moore",
      "Marylyn Ritchie",
      "Li Shen"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics."
  },
  {
    "url": "http://arxiv.org/abs/2507.23220v1",
    "title": "Model Directions, Not Words: Mechanistic Topic Models Using Sparse   Autoencoders",
    "authors": [
      "Carolina Zheng",
      "Nicolas Beltran-Velez",
      "Sweta Karlekar",
      "Claudia Shi",
      "Achille Nazaret",
      "Asif Mallik",
      "Amir Feder",
      "David M. Blei"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \\textit{topic judge}, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs."
  },
  {
    "url": "http://arxiv.org/abs/2507.23211v1",
    "title": "Failures Are the Stepping Stones to Success: Enhancing Few-Shot   In-Context Learning by Leveraging Negative Samples",
    "authors": [
      "Yunhao Liang",
      "Ruixuan Ying",
      "Takuya Taniguchi",
      "Zhe Cui"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models exhibit powerful few-shot in-context learning (ICL) capabilities, but the performance is highly sensitive to provided examples."
  },
  {
    "url": "http://arxiv.org/abs/2507.23194v1",
    "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks",
    "authors": [
      "Jianghui Wang",
      "Vinay Joshi",
      "Saptarshi Majumder",
      "Xu Chao",
      "Bin Ding",
      "Ziqiong Liu",
      "Pratik Prabhanjan Brahma",
      "Dong Li",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance."
  },
  {
    "url": "http://arxiv.org/abs/2507.23167v1",
    "title": "LENS: Learning Ensemble Confidence from Neural States for Multi-LLM   Answer Integration",
    "authors": [
      "Jizhou Guo"
    ],
    "date": "2025-07-31",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have demonstrated impressive performance across various tasks, with different models excelling in distinct domains and specific abilities. Effectively combining the predictions of multiple LLMs is crucial for enhancing system robustness and performance. However, existing ensemble methods often rely on simple techniques like voting or logits ensembling, which overlook the varying confidence and reliability of models in different contexts. In this work, we propose LENS (Learning ENsemble confidence from Neural States), a novel approach that learns to estimate model confidence by analyzing internal representations. For each LLM, we train a lightweight linear confidence predictor that leverages layer-wise hidden states and normalized probabilities as inputs. This allows for more nuanced weighting of model predictions based on their context-dependent reliability. Our method does not require modifying the model parameters and requires negligible additional computation. Experimental results on multiple-choice and boolean question-answering tasks demonstrate that LENS outperforms traditional ensemble methods by a substantial margin. Our findings suggest that internal representations provide valuable signals for determining model confidence and can be effectively leveraged for ensemble learning."
  },
  {
    "url": "http://arxiv.org/abs/2507.23158v1",
    "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But   Noisy as a Learning Signal",
    "authors": [
      "Yuhan Liu",
      "Michael J. Q. Zhang",
      "Eunsol Choi"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the user's initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations."
  },
  {
    "url": "http://arxiv.org/abs/2507.23135v1",
    "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language   Models through Procedural Plans",
    "authors": [
      "Ananya Sadana",
      "Yash Kumar Lal",
      "Jiawei Zhou"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Understanding causal relationships across modalities is a core challenge for multimodal models operating in real-world environments. We introduce ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text. Each example presents an image of a task step and a text snippet from a plan, with the goal of deciding whether the visual step occurs before or after the referenced text step. Evaluation results on ten frontier vision-language models show underwhelming performance: the best zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further highlights concrete directions for improving causal understanding in multimodal models."
  },
  {
    "url": "http://arxiv.org/abs/2507.23121v1",
    "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual   Ambiguity",
    "authors": [
      "Xinwei Wu",
      "Haojie Li",
      "Hongyu Liu",
      "Xinyu Ji",
      "Ruohan Li",
      "Yule Chen",
      "Yigeng Zhang"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.23104v1",
    "title": "RASL: Retrieval Augmented Schema Linking for Massive Database   Text-to-SQL",
    "authors": [
      "Jeffrey Eben",
      "Aitzaz Ahmad",
      "Stephen Lau"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces."
  },
  {
    "url": "http://arxiv.org/abs/2507.23095v1",
    "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with   Structural Integrity",
    "authors": [
      "Ishani Mondal",
      "Meera Bharadwaj",
      "Ayush Roy",
      "Aparna Garimella",
      "Jordan Lee Boyd-Graber"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits."
  },
  {
    "url": "http://arxiv.org/abs/2507.23083v1",
    "title": "Context-aware Rotary Position Embedding",
    "authors": [
      "Ali Veisi",
      "Delaram Fartoot",
      "Hamidreza Amirzadeh"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Positional encoding is a vital component of Transformer architectures, enabling models to incorporate sequence order into self-attention mechanisms. Rotary Positional Embeddings (RoPE) have become a widely adopted solution due to their compatibility with relative position encoding and computational efficiency. However, RoPE relies on static, input-independent sinusoidal frequency patterns, limiting its ability to model context-sensitive relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional Embedding), a novel generalization of RoPE that dynamically generates head-specific frequency patterns conditioned on token embeddings. This design introduces token- and context-sensitive positional representations while preserving RoPE efficiency and architectural simplicity. CARoPE computes input-dependent phase shifts using a bounded transformation of token embeddings and integrates them into the rotary mechanism across attention heads. We evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on next-token prediction tasks. Experimental results show that CARoPE consistently outperforms RoPE and other common positional encoding baselines, achieving significantly lower perplexity, even at longer context lengths. Additionally, CARoPE enables faster training throughput without sacrificing model stability. These findings demonstrate that CARoPE offers a scalable, expressive, and efficient upgrade to existing positional encoding strategies in Transformer models."
  },
  {
    "url": "http://arxiv.org/abs/2507.23082v1",
    "title": "Exploring In-Context Learning for Frame-Semantic Parsing",
    "authors": [
      "Diego Garat",
      "Guillermo Moncecchi",
      "Dina Wonsever"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Frame Semantic Parsing (FSP) entails identifying predicates and labeling their arguments according to Frame Semantics. This paper investigates the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP without model fine-tuning. We propose a method that automatically generates task-specific prompts for the Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks, relying solely on the FrameNet database. These prompts, constructed from frame definitions and annotated examples, are used to guide six different LLMs. Experiments are conducted on a subset of frames related to violent events. The method achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers a practical and effective alternative to traditional fine-tuning for domain-specific FSP tasks."
  },
  {
    "url": "http://arxiv.org/abs/2507.23063v1",
    "title": "Math Natural Language Inference: this should be easy!",
    "authors": [
      "Valeria de Paiva",
      "Qiyue Gao",
      "Hai Hu",
      "Pavel Kovalev",
      "Yikang Liu",
      "Lawrence S. Moss",
      "Zhiheng Qian"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:We ask whether contemporary LLMs are able to perform natural language inference (NLI) tasks on mathematical texts. We call this the Math NLI problem. We construct a corpus of Math NLI pairs whose premises are from extant mathematical text and whose hypotheses and gold labels were provided by people with experience in both research-level mathematics and also in the NLI field. We also investigate the quality of corpora using the same premises but whose hypotheses are provided by LLMs themselves. We not only investigate the performance but also the inter-group consistency of the diverse group of LLMs. We have both positive and negative findings. Among our positive findings: in some settings, using a majority vote of LLMs is approximately equivalent to using human-labeled data in the Math NLI area. On the negative side: LLMs still struggle with mathematical language. They occasionally fail at even basic inferences. Current models are not as prone to hypothesis-only \"inference\" in our data the way the previous generation had been. In addition to our findings, we also provide our corpora as data to support future work on Math NLI."
  },
  {
    "url": "http://arxiv.org/abs/2507.22887v1",
    "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context   Learning",
    "authors": [
      "Kwesi Cobbina",
      "Tianyi Zhou"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks."
  },
  {
    "url": "http://arxiv.org/abs/2507.22968v1",
    "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring   Challenges in Complex Conversations",
    "authors": [
      "Chengqian Ma",
      "Wei Tao",
      "Yiwen Guo"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges."
  },
  {
    "url": "http://arxiv.org/abs/2507.22879v2",
    "title": "RecGPT Technical Report",
    "authors": [
      "Chao Yi",
      "Dian Chen",
      "Gaoyang Guo",
      "Jiakai Tang",
      "Jian Wu",
      "Jing Yu",
      "Mao Zhang",
      "Sunhao Dai",
      "Wen Chen",
      "Wenjun Yang",
      "Yuning Jiang",
      "Zhujin Gao",
      "Bo Zheng",
      "Chi Li",
      "Dimin Wang",
      "Dixuan Wang",
      "Fan Li",
      "Fan Zhang",
      "Haibin Chen",
      "Haozhuang Liu",
      "Jialin Zhu",
      "Jiamang Wang",
      "Jiawei Wu",
      "Jin Cui",
      "Ju Huang",
      "Kai Zhang",
      "Kan Liu",
      "Lang Tian",
      "Liang Rao",
      "Longbin Li",
      "Lulu Zhao",
      "Na He",
      "Peiyang Wang",
      "Qiqi Huang",
      "Tao Luo",
      "Wenbo Su",
      "Xiaoxiao He",
      "Xin Tong",
      "Xu Chen",
      "Xunke Xi",
      "Yang Li",
      "Yaxuan Wu",
      "Yeqiu Yang",
      "Yi Hu",
      "Yinnan Song",
      "Yuchen Li",
      "Yujie Luo",
      "Yujin Yuan",
      "Yuliang Yan",
      "Zhengyang Wang",
      "Zhibo Xiao",
      "Zhixin Ma",
      "Zile Zhou",
      "Ziqi Zhang"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem."
  },
  {
    "url": "http://arxiv.org/abs/2507.22878v1",
    "title": "GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for   Multiresolution Power Outage Analysis",
    "authors": [
      "Ethan Frakes",
      "Yinghui Wu",
      "Roger H. French",
      "Mengjie Li"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Detecting, analyzing, and predicting power outages is crucial for grid risk assessment and disaster mitigation. Numerous outages occur each year, exacerbated by extreme weather events such as hurricanes. Existing outage data are typically reported at the county level, limiting their spatial resolution and making it difficult to capture localized patterns. However, it offers excellent temporal granularity. In contrast, nighttime light satellite image data provides significantly higher spatial resolution and enables a more comprehensive spatial depiction of outages, enhancing the accuracy of assessing the geographic extent and severity of power loss after disaster events. However, these satellite data are only available on a daily basis. Integrating spatiotemporal visual and time-series data sources into a unified knowledge representation can substantially improve power outage detection, analysis, and predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S. We describe our method for constructing GeoOutageKG by aligning source data with a developed ontology, GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual outage records spanning from 2014 to 2024, 300,000 NTL images spanning from 2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration. We demonstrate its use through multiresolution analysis of geospatiotemporal power outages."
  },
  {
    "url": "http://arxiv.org/abs/2507.22847v1",
    "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology",
    "authors": [
      "Han Jiang",
      "Pengda Wang",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Ziang Xiao"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems."
  },
  {
    "url": "http://arxiv.org/abs/2507.22829v1",
    "title": "Beyond Natural Language Plans: Structure-Aware Planning for   Query-Focused Table Summarization",
    "authors": [
      "Weijia Zhang",
      "Songgaojun Deng",
      "Evangelos Kanoulas"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Query-focused table summarization requires complex reasoning, often approached through step-by-step natural language (NL) plans. However, NL plans are inherently ambiguous and lack structure, limiting their conversion into executable programs like SQL and hindering scalability, especially for multi-table tasks. To address this, we propose a paradigm shift to structured representations. We introduce a new structured plan, TaSoF, inspired by formalism in traditional multi-agent systems, and a framework, SPaGe, that formalizes the reasoning process in three phases: 1) Structured Planning to generate TaSoF from a query, 2) Graph-based Execution to convert plan steps into SQL and model dependencies via a directed cyclic graph for parallel execution, and 3) Summary Generation to produce query-focused summaries. Our method explicitly captures complex dependencies and improves reliability. Experiments on three public benchmarks show that SPaGe consistently outperforms prior models in both single- and multi-table settings, demonstrating the advantages of structured representations for robust and scalable summarization."
  },
  {
    "url": "http://arxiv.org/abs/2507.22811v1",
    "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph",
    "authors": [
      "Debayan Banerjee",
      "Tilahun Abedissa Taffa",
      "Ricardo Usbeck"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the \"yes\" token output at the penultimate layer of the LLM."
  },
  {
    "url": "http://arxiv.org/abs/2507.22758v1",
    "title": "MASCA: LLM based-Multi Agents System for Credit Assessment",
    "authors": [
      "Gautam Jajoo",
      "Pranjal A Chitale",
      "Saksham Agarwal"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring."
  },
  {
    "url": "http://arxiv.org/abs/2507.22753v1",
    "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective",
    "authors": [
      "Sowmya Vajjala",
      "Bashar Alhafni",
      "Stefano Bann\u00f2",
      "Kaushal Kumar Maurya",
      "Ekaterina Kochmar"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Interest in the role of large language models (LLMs) in education is increasing, considering the new opportunities they offer for teaching, learning, and assessment. In this paper, we examine the impact of LLMs on educational NLP in the context of two main application scenarios: {\\em assistance} and {\\em assessment}, grounding them along the four dimensions -- reading, writing, speaking, and tutoring. We then present the new directions enabled by LLMs, and the key challenges to address. We envision that this holistic overview would be useful for NLP researchers and practitioners interested in exploring the role of LLMs in developing language-focused and NLP-enabled educational applications of the future."
  },
  {
    "url": "http://arxiv.org/abs/2507.22752v1",
    "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset",
    "authors": [
      "Jind\u0159ich Libovick\u00fd",
      "Jind\u0159ich Helcl",
      "Andrei Manea",
      "Gianluca Vico"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:We introduce a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. We also provide strong baselines using state-of-the-art large language models (LLMs). Our dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations. It includes both purely textual questions and those requiring visual understanding. As a baseline, we evaluate state-of-the-art LLMs through prompting and complement this with human judgments of answer correctness. Using these human evaluations, we analyze the reliability of existing automatic evaluation metrics. Our baseline results highlight a significant gap in regional knowledge among current LLMs. Moreover, apart from LLM-based evaluation, there is minimal correlation between automated metrics and human judgment. We release this dataset as a resource to (1) assess regional knowledge in LLMs, (2) study cross-lingual generation consistency in a challenging setting, and (3) advance the development of evaluation metrics for open-ended question answering."
  },
  {
    "url": "http://arxiv.org/abs/2507.22746v2",
    "title": "Next Tokens Denoising for Speech Synthesis",
    "authors": [
      "Yanqing Liu",
      "Ruiqing Xue",
      "Chong Zhang",
      "Yufei Liu",
      "Gang Wang",
      "Bohan Li",
      "Yao Qian",
      "Lei He",
      "Shujie Liu",
      "Sheng Zhao"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:While diffusion and autoregressive (AR) models have significantly advanced generative modeling, they each present distinct limitations. AR models, which rely on causal attention, cannot exploit future context and suffer from slow generation speeds. Conversely, diffusion models struggle with key-value (KV) caching. To overcome these challenges, we introduce Dragon-FM, a novel text-to-speech (TTS) design that unifies AR and flow-matching. This model processes 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens per second. This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Thus, the model leverages KV-cache across chunks and utilizes bidirectional context within each chunk. Furthermore, it bridges continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers. This efficient codec and fast chunk-autoregressive architecture also make the model highly effective for generating long-form content, such as podcasts. Experiments on podcast datasets demonstrate its capability to efficiently generate high-quality zero-shot podcasts."
  },
  {
    "url": "http://arxiv.org/abs/2507.22744v1",
    "title": "Reducing Hallucinations in Summarization via Reinforcement Learning with   Entity Hallucination Index",
    "authors": [
      "Praveenkumar Katwe",
      "Rakesh Chandra",
      "Balabantaray Kali",
      "Prasad Vittala"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI."
  },
  {
    "url": "http://arxiv.org/abs/2507.22729v1",
    "title": "Resource-Efficient Adaptation of Large Language Models for Text   Embeddings via Prompt Engineering and Contrastive Fine-tuning",
    "authors": [
      "Benedikt Roth",
      "Stephan Rappensperger",
      "Tianming Qiu",
      "Hamza Imamovi\u0107",
      "Julian W\u00f6rmann",
      "Hao Shen"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs."
  },
  {
    "url": "http://arxiv.org/abs/2507.22720v1",
    "title": "Investigating Hallucination in Conversations for Low Resource Languages",
    "authors": [
      "Amit Das",
      "Md. Najib Hasan",
      "Souvika Sarkar",
      "Zheng Zhang",
      "Fatemeh Jamshidi",
      "Tathagata Bhattacharya",
      "Nilanjana Raychawdhury",
      "Dongji Feng",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi."
  },
  {
    "url": "http://arxiv.org/abs/2507.22716v1",
    "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in   Retrieval-Augmented Reasoning for LLMs",
    "authors": [
      "Jie He",
      "Victor Gutierrez Basulto",
      "Jeff Z. Pan"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.22676v1",
    "title": "Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview   Performance Assessment",
    "authors": [
      "Jia Li",
      "Yang Wang",
      "Wenhao Qian",
      "Zhenzhen Hu",
      "Richang Hong",
      "Meng Wang"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Interview performance assessment is essential for determining candidates' suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores ``365'' aspects of interview performance by integrating \\textit{three} modalities (video, audio, and text), \\textit{six} responses per candidate, and \\textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2508.00033v1",
    "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel   Python Libraries",
    "authors": [
      "Nuno Fachada",
      "Daniel Fernandes",
      "Carlos M. Fernandes",
      "Bruno D. Ferreira-Saraiva",
      "Jo\u00e3o P. Matos-Carvalho"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \\textit{ParShift} library, and synthetic data generation and clustering using \\textit{pyclugen} and \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code, with GPT-4.1 standing out as the only model to always succeed in both tasks. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities."
  },
  {
    "url": "http://arxiv.org/abs/2507.22623v1",
    "title": "Multilingual Political Views of Large Language Models: Identification   and Steering",
    "authors": [
      "Daniil Gurgurov",
      "Katharina Trinley",
      "Ivan Vykopal",
      "Josef van Genabith",
      "Simon Ostermann",
      "Roberto Zamparelli"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled."
  },
  {
    "url": "http://arxiv.org/abs/2507.22608v1",
    "title": "Language Arithmetics: Towards Systematic Language Neuron Identification   and Manipulation",
    "authors": [
      "Daniil Gurgurov",
      "Katharina Trinley",
      "Yusser Al Ghussin",
      "Tanja Baeumel",
      "Josef van Genabith",
      "Simon Ostermann"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity."
  },
  {
    "url": "http://arxiv.org/abs/2507.22607v2",
    "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced   Multimodal Reasoning",
    "authors": [
      "Ruifeng Yuan",
      "Chenghao Xiao",
      "Sicong Leng",
      "Jianyu Wang",
      "Long Li",
      "Weiwen Xu",
      "Hou Pong Chan",
      "Deli Zhao",
      "Tingyang Xu",
      "Zhongyu Wei",
      "Hao Zhang",
      "Yu Rong"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach."
  },
  {
    "url": "http://arxiv.org/abs/2507.22603v1",
    "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models",
    "authors": [
      "Rawan Al-Matham",
      "Kareem Darwish",
      "Raghad Al-Rasheed",
      "Waad Alshammari",
      "Muneera Alhoshan",
      "Amal Almazrua",
      "Asma Al Wazrah",
      "Mais Alheraki",
      "Firoj Alam",
      "Preslav Nakov",
      "Norah Alzahrani",
      "Eman alBilali",
      "Nizar Habash",
      "Abdelrahman El-Sheikh",
      "Muhammad Elmallah",
      "Haonan Li",
      "Hamdy Mubarak",
      "Mohamed Anwar",
      "Zaid Alyafeai",
      "Ahmed Abdelali",
      "Nora Altwairesh",
      "Maram Hasanain",
      "Abdulmohsen Al Thubaity",
      "Shady Shehata",
      "Bashar Alhafni",
      "Injy Hamed",
      "Go Inoue",
      "Khalid Elmadani",
      "Ossama Obeid",
      "Fatima Haouari",
      "Tamer Elsayed",
      "Emad Alghamdi",
      "Khalid Almubarak",
      "Saied Alshahrani",
      "Ola Aljarrah",
      "Safa Alajlan",
      "Areej Alshaqarawi",
      "Maryam Alshihri",
      "Sultana Alghurabi",
      "Atikah Alzeghayer",
      "Afrah Altamimi",
      "Abdullah Alfaifi",
      "Abdulrahman AlOsaimy"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities."
  },
  {
    "url": "http://arxiv.org/abs/2507.22581v2",
    "title": "Unveiling the Influence of Amplifying Language-Specific Neurons",
    "authors": [
      "Inaya Rahmanisa",
      "Lyzander Marciano Andrylie",
      "Mahardika Krisna Ihsani",
      "Alfan Farizki Wicaksono",
      "Haryo Akbarianto Wibowo",
      "Alham Fikri Aji"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer."
  },
  {
    "url": "http://arxiv.org/abs/2507.22565v1",
    "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement   Learning",
    "authors": [
      "Afshin Khadangi",
      "Amir Sartipi",
      "Igor Tchappi",
      "Ramin Bahmani",
      "Gilbert Fridgen"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks."
  },
  {
    "url": "http://arxiv.org/abs/2507.22564v1",
    "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs",
    "authors": [
      "Xikang Yang",
      "Biyu Zhou",
      "Xuehai Tang",
      "Jizhong Han",
      "Songlin Hu"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems."
  },
  {
    "url": "http://arxiv.org/abs/2507.22545v2",
    "title": "ControlMed: Adding Reasoning Control to Medical Language Model",
    "authors": [
      "Sung-Min Lee",
      "Siyoon Lee",
      "Juyeon Kim",
      "Kyoungmin Roh"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \\textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \\textit{direct} and \\textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis."
  },
  {
    "url": "http://arxiv.org/abs/2507.22543v1",
    "title": "Pre-trained Models Perform the Best When Token Distributions Follow   Zipf's Law",
    "authors": [
      "Yanjin He",
      "Qingkai Zeng",
      "Meng Jiang"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Tokenization is a fundamental step in natural language processing (NLP) and other sequence modeling domains, where the choice of vocabulary size significantly impacts model performance. Despite its importance, selecting an optimal vocabulary size remains underexplored, typically relying on heuristics or dataset-specific choices. In this work, we propose a principled method for determining the vocabulary size by analyzing token frequency distributions through Zipf's law. We show that downstream task performance correlates with how closely token distributions follow power-law behavior, and that aligning with Zipfian scaling improves both model efficiency and effectiveness. Extensive experiments across NLP, genomics, and chemistry demonstrate that models consistently achieve peak performance when the token distribution closely adheres to Zipf's law, establishing Zipfian alignment as a robust and generalizable criterion for vocabulary size selection."
  },
  {
    "url": "http://arxiv.org/abs/2507.22542v1",
    "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large   Language Models in Customer Support",
    "authors": [
      "Long S. T. Nguyen",
      "Truong P. Hua",
      "Thanh M. Nguyen",
      "Toan Q. Pham",
      "Nam K. Ngo",
      "An X. Nguyen",
      "Nghi D. M. Pham",
      "Nghia H. Nguyen",
      "Tho T. Quan"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:With the rapid growth of Artificial Intelligence, Large Language Models (LLMs) have become essential for Question Answering (QA) systems, improving efficiency and reducing human workload in customer service. The emergence of Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a practical choice for their accuracy, efficiency, and privacy benefits. However, domain-specific evaluations remain limited, and the absence of benchmark datasets reflecting real customer interactions makes it difficult for enterprises to select suitable models for support applications. To address this gap, we introduce the Customer Support Conversations Dataset (CSConDa), a curated benchmark of over 9,000 QA pairs drawn from real interactions with human advisors at a large Vietnamese software company. Covering diverse topics such as pricing, product availability, and technical troubleshooting, CSConDa provides a representative basis for evaluating ViLLMs in practical scenarios. We further present a comprehensive evaluation framework, benchmarking 11 lightweight open-source ViLLMs on CSConDa with both automatic metrics and syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns. This study offers insights into model behavior, explains performance differences, and identifies key areas for improvement, supporting the development of next-generation ViLLMs. By establishing a robust benchmark and systematic evaluation, our work enables informed model selection for customer service QA and advances research on Vietnamese LLMs. The dataset is publicly available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.22533v1",
    "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for   Decision Support over Longitudinal Cancer Electronic Health Records",
    "authors": [
      "Dongchen Li",
      "Jitao Liang",
      "Wei Li",
      "Xiaoyu Wang",
      "Longbing Cao",
      "Kun Yu"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists."
  },
  {
    "url": "http://arxiv.org/abs/2507.22478v1",
    "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL",
    "authors": [
      "Lei Sheng",
      "Shuai-Shuai Xu"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy (EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset, model, and code to github: this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.22964v1",
    "title": "Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR",
    "authors": [
      "Sotheara Leang",
      "\u00c9ric Castelli",
      "Dominique Vaufreydaz",
      "Sethserey Sam"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:The dynamic characteristics of speech signal provides temporal information and play an important role in enhancing Automatic Speech Recognition (ASR). In this work, we characterized the acoustic transitions in a ratio plane of Spectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture the dynamic characteristics of the speech and minimize spectral variation. These dynamic parameters were combined with Mel-Frequency Cepstral Coefficients (MFCCs) in Vietnamese ASR to capture more detailed spectral information. The SSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to describe the tonal information robustly. The findings showed that the proposed parameters significantly reduce word error rates and exhibit greater gender independence than the baseline MFCCs."
  },
  {
    "url": "http://arxiv.org/abs/2507.22462v2",
    "title": "IFEvalCode: Controlled Code Generation",
    "authors": [
      "Jian Yang",
      "Wei Zhang",
      "Shukai Liu",
      "Linzheng Chai",
      "Yingshui Tan",
      "Jiaheng Liu",
      "Ge Zhang",
      "Wangchunshu Zhou",
      "Guanglin Niu",
      "Zhoujun Li",
      "Binyuan Hui",
      "Junyang Lin"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions."
  },
  {
    "url": "http://arxiv.org/abs/2507.22457v1",
    "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments   about Large Language Models",
    "authors": [
      "Tian Yun",
      "Chen Sun",
      "Ellie Pavlick"
    ],
    "date": "2025-07-30",
    "abstract": "Title:What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models"
  },
  {
    "url": "http://arxiv.org/abs/2507.22448v1",
    "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance",
    "authors": [
      "Jingwei Zuo",
      "Maksim Velikanov",
      "Ilyas Chahed",
      "Younes Belkada",
      "Dhia Eddine Rhayem",
      "Guillaume Kunsch",
      "Hakim Hacid",
      "Hamza Yous",
      "Brahim Farhat",
      "Ibrahim Khadraoui",
      "Mugariya Farooq",
      "Giulia Campesan",
      "Ruxandra Cojocaru",
      "Yasser Djilali",
      "Shi Hu",
      "Iheb Chaabane",
      "Puneesh Khanna",
      "Mohamed El Amine Seddik",
      "Ngoc Dung Huynh",
      "Phuc Le Khac",
      "Leen AlQadi",
      "Billel Mokeddem",
      "Mohamed Chami",
      "Abdalgader Abubaker",
      "Mikhail Lubinets",
      "Kacper Piskorski",
      "Slim Frikha"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research."
  },
  {
    "url": "http://arxiv.org/abs/2507.22445v1",
    "title": "AI-generated stories favour stability over change: homogeneity and   cultural stereotyping in narratives generated by gpt-4o-mini",
    "authors": [
      "Jill Walker Rettberg",
      "Hermann Wigers"
    ],
    "date": "2025-07-30",
    "abstract": "View PDFAbstract:Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a 1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI."
  },
  {
    "url": "http://arxiv.org/abs/2507.22411v1",
    "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large   Language Models",
    "authors": [
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \\textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at this https URL"
  },
  {
    "url": "http://arxiv.org/abs/2507.22410v1",
    "title": "Question Generation for Assessing Early Literacy Reading Comprehension",
    "authors": [
      "Xiaocheng Yang",
      "Sumuk Shashidhar",
      "Dilek Hakkani-Tur"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors."
  },
  {
    "url": "http://arxiv.org/abs/2507.22387v1",
    "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs",
    "authors": [
      "Homaira Huda Shomee",
      "Suman Kalyan Maity",
      "Sourav Medya"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research."
  },
  {
    "url": "http://arxiv.org/abs/2507.22367v1",
    "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided   LLM Representations and Multimodal Apparent Behaviors",
    "authors": [
      "Jia Li",
      "Yichao He",
      "Jiacheng Xu",
      "Tianhao Luo",
      "Zhenzhen Hu",
      "Richang Hong",
      "Meng Wang"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors, with asynchronous patterns across modalities. It was hard to model personality semantics with traditional superficial features and seemed impossible to achieve effective cross-modal understanding. To address these challenges, we propose a novel personality assessment framework called \\textit{\\textbf{Traits Run Deep}}. It employs \\textit{\\textbf{psychology-informed prompts}} to elicit high-level personality-relevant semantic representations. Besides, it devises a \\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text semantics to align and integrate asynchronous signals from other modalities. To be specific, such fusion module includes a Chunk-Wise Projector to decrease dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for effective modality fusion and an ensemble regression head to improve generalization in data-scarce situations. To our knowledge, we are the first to apply personality-specific prompts to guide large language models (LLMs) in extracting personality-aware semantics for improved representation quality. Furthermore, extracting and fusing audio-visual apparent behavior features further improves the accuracy. Experimental results on the AVI validation set have demonstrated the effectiveness of the proposed components, i.e., approximately a 45\\% reduction in mean squared error (MSE). Final evaluations on the test set of the AVI Challenge 2025 confirm our method's superiority, ranking first in the Personality Assessment track. The source code will be made available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.22359v2",
    "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of   Large Language Models",
    "authors": [
      "Qianhong Guo",
      "Wei Xie",
      "Xiaofang Cai",
      "Enze Wang",
      "Shuoyoucheng Ma",
      "Kai Chen",
      "Xiaofeng Wang",
      "Baosheng Wang"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness)."
  },
  {
    "url": "http://arxiv.org/abs/2508.00028v1",
    "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework   and ITU-R Propagation Models",
    "authors": [
      "Abir Ray"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Spectrum resources are often underutilized across time and space, motivating dynamic spectrum access strategies that allow secondary users to exploit unused frequencies. A key challenge is predicting when and where spectrum will be available (i.e., unused by primary licensed users) in order to enable proactive and interference-free access. This paper proposes a scalable framework for spectrum availability prediction that combines a two-state Markov chain model of primary user activity with high-fidelity propagation models from the ITU-R (specifically Recommendations P.528 and P.2108). The Markov chain captures temporal occupancy patterns, while the propagation models incorporate path loss and clutter effects to determine if primary signals exceed interference thresholds at secondary user locations. By integrating these components, the proposed method can predict spectrum opportunities both in time and space with improved accuracy. We develop the system model and algorithm for the approach, analyze its scalability and computational efficiency, and discuss assumptions, limitations, and potential applications. The framework is flexible and can be adapted to various frequency bands and scenarios. The results and analysis show that the proposed approach can effectively identify available spectrum with low computational cost, making it suitable for real-time spectrum management in cognitive radio networks and other dynamic spectrum sharing systems."
  },
  {
    "url": "http://arxiv.org/abs/2507.22337v1",
    "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers",
    "authors": [
      "Roxana Petcu",
      "Samarth Bhargav",
      "Maarten de Rijke",
      "Evangelos Kanoulas"
    ],
    "date": "2025-07-30",
    "abstract": "View PDF HTML (experimental)Abstract:Understanding and solving complex reasoning tasks is vital for addressing the information needs of a user. Although dense neural models learn contextualised embeddings, they still underperform on queries containing negation. To understand this phenomenon, we study negation in both traditional neural information retrieval and LLM-based models. We (1) introduce a taxonomy of negation that derives from philosophical, linguistic, and logical definitions; (2) generate two benchmark datasets that can be used to evaluate the performance of neural information retrieval models and to fine-tune models for a more robust performance on negation; and (3) propose a logic-based classification mechanism that can be used to analyze the performance of retrieval models on existing datasets. Our taxonomy produces a balanced data distribution over negation types, providing a better training setup that leads to faster convergence on the NevIR dataset. Moreover, we propose a classification schema that reveals the coverage of negation types in existing datasets, offering insights into the factors that might affect the generalization of fine-tuned models on negation."
  },
  {
    "url": "http://arxiv.org/abs/2507.22289v1",
    "title": "Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party   Conversations",
    "authors": [
      "Galo Castillo-L\u00f3pez",
      "Ga\u00ebl de Chalendar",
      "Nasredine Semmar"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Intent recognition is a fundamental component in task-oriented dialogue systems (TODS). Determining user intents and detecting whether an intent is Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However, traditional TODS require large amount of annotated data. In this work we propose a hybrid approach to combine BERT and LLMs in zero and few-shot settings to recognize intents and detect OOS utterances. Our approach leverages LLMs generalization power and BERT's computational efficiency in such scenarios. We evaluate our method on multi-party conversation corpora and observe that sharing information from BERT outputs to LLMs leads to system performance improvement."
  },
  {
    "url": "http://arxiv.org/abs/2507.22286v1",
    "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric   Representations of Constructions in LLMs",
    "authors": [
      "Supantho Rakshit",
      "Adele Goldberg"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:The usage-based constructionist (UCx) approach posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze the neural representations of the English dative constructions (Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied for human-rated preference strength. A macro-level geometric analysis finds that the separability between construction representations, as measured by Energy Distance or Jensen-Shannon Divergence, is systematically modulated by gradient preference strength. More prototypical exemplars of each construction occupy more distinct regions in the activation space of LLMs. These results provide strong evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures of basic constructionist principles in LLMs."
  },
  {
    "url": "http://arxiv.org/abs/2507.22281v1",
    "title": "CoEx -- Co-evolving World-model and Exploration",
    "authors": [
      "Minsoo Kim",
      "Seung-won Hwang"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration."
  },
  {
    "url": "http://arxiv.org/abs/2507.22219v1",
    "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine   Translation",
    "authors": [
      "Dongyub Jude Lee",
      "Zhenyi Ye",
      "Pengcheng He"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores."
  },
  {
    "url": "http://arxiv.org/abs/2507.22209v1",
    "title": "How Well Does First-Token Entropy Approximate Word Entropy as a   Psycholinguistic Predictor?",
    "authors": [
      "Christian Clark",
      "Byung-Doh Oh",
      "William Schuler"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Contextual entropy is a psycholinguistic measure capturing the anticipated difficulty of processing a word just before it is encountered. Recent studies have tested for entropy-related effects as a potential complement to well-known effects from surprisal. For convenience, entropy is typically estimated based on a language model's probability distribution over a word's first subword token. However, this approximation results in underestimation and potential distortion of true word entropy. To address this, we generate Monte Carlo (MC) estimates of word entropy that allow words to span a variable number of tokens. Regression experiments on reading times show divergent results between first-token and MC word entropy, suggesting a need for caution in using first-token approximations of contextual entropy."
  },
  {
    "url": "http://arxiv.org/abs/2507.22201v1",
    "title": "The role of media memorability in facilitating startups' access to   venture capital funding",
    "authors": [
      "L. Toschi",
      "S. Torrisi",
      "A. Fronzetti Colladon"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation."
  },
  {
    "url": "http://arxiv.org/abs/2507.22197v1",
    "title": "Explainability Through Systematicity: The Hard Systematicity Challenge   for Artificial Intelligence",
    "authors": [
      "Matthieu Queloz"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity--not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the \"systematicity challenge\" to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed \"the systematicity of thought.\" I offer a conceptual framework for thinking about \"the systematicity of thought\" that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the \"hard systematicity challenge.\" However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when."
  },
  {
    "url": "http://arxiv.org/abs/2507.22187v1",
    "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large   Language Models",
    "authors": [
      "Adam M. Morgan",
      "Adeen Flinker"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research."
  },
  {
    "url": "http://arxiv.org/abs/2507.22168v1",
    "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing   Styles",
    "authors": [
      "Kimberly Le Truong",
      "Riccardo Fogliato",
      "Hoda Heidari",
      "Zhiwei Steven Wu"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations."
  },
  {
    "url": "http://arxiv.org/abs/2507.22160v1",
    "title": "Strategic Deflection: Defending LLMs from Logit Manipulation",
    "authors": [
      "Yassine Rachidy",
      "Jihad Rbaiti",
      "Youssef Hmamouche",
      "Faissal Sehbaoui",
      "Amal El Fallah Seghrouchni"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:With the growing adoption of Large Language Models (LLMs) in critical areas, ensuring their security against jailbreaking attacks is paramount. While traditional defenses primarily rely on refusing malicious prompts, recent logit-level attacks have demonstrated the ability to bypass these safeguards by directly manipulating the token-selection process during generation. We introduce Strategic Deflection (SDeflection), a defense that redefines the LLM's response to such advanced attacks. Instead of outright refusal, the model produces an answer that is semantically adjacent to the user's request yet strips away the harmful intent, thereby neutralizing the attacker's harmful intent. Our experiments demonstrate that SDeflection significantly lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This work presents a critical shift in defensive strategies, moving from simple refusal to strategic content redirection to neutralize advanced threats."
  },
  {
    "url": "http://arxiv.org/abs/2507.22159v1",
    "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian",
    "authors": [
      "Vanessa Rebecca Wiyono",
      "David Anugraha",
      "Ayu Purwarianti",
      "Genta Indra Winata"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset specifically designed to evaluate the naturalness and quality of LLM-generated text. All annotations are natively written in Indonesian and evaluated using Krippendorff's alpha, demonstrating strong inter-annotator agreement. Additionally, we benchmark the dataset across multiple LLMs and assess the output quality of each model."
  },
  {
    "url": "http://arxiv.org/abs/2507.22133v1",
    "title": "Prompt Optimization and Evaluation for LLM Automated Red Teaming",
    "authors": [
      "Michael Freenor",
      "Lauren Alvarez",
      "Milton Leal",
      "Lily Smith",
      "Joel Garrett",
      "Yelyzaveta Husieva",
      "Madeline Woodruff",
      "Ryan Miller",
      "Erich Kummerfeld",
      "Rafael Medeiros",
      "Sander Schulhoff"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Applications that use Large Language Models (LLMs) are becoming widespread, making the identification of system vulnerabilities increasingly important. Automated Red Teaming accelerates this effort by using an LLM to generate and execute attacks against target systems. Attack generators are evaluated using the Attack Success Rate (ASR) the sample mean calculated over the judgment of success for each attack. In this paper, we introduce a method for optimizing attack generator prompts that applies ASR to individual attacks. By repeating each attack multiple times against a randomly seeded target, we measure an attack's discoverability the expectation of the individual attack success. This approach reveals exploitable patterns that inform prompt optimization, ultimately enabling more robust evaluation and refinement of generators."
  },
  {
    "url": "http://arxiv.org/abs/2507.22062v3",
    "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
    "authors": [
      "Yung-Sung Chuang",
      "Yang Li",
      "Dong Wang",
      "Ching-Feng Yeh",
      "Kehan Lyu",
      "Ramya Raghavendra",
      "James Glass",
      "Lifei Huang",
      "Jason Weston",
      "Luke Zettlemoyer",
      "Xinlei Chen",
      "Zhuang Liu",
      "Saining Xie",
      "Wen-tau Yih",
      "Shang-Wen Li",
      "Hu Xu"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval."
  },
  {
    "url": "http://arxiv.org/abs/2507.22050v2",
    "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router",
    "authors": [
      "Minghao Guo",
      "Qingcheng Zeng",
      "Xujiang Zhao",
      "Yanchi Liu",
      "Wenchao Yu",
      "Mengnan Du",
      "Haifeng Chen",
      "Wei Cheng"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches. Our codes are available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.22034v1",
    "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
    "authors": [
      "Cheng Qian",
      "Zuxin Liu",
      "Akshara Prabhakar",
      "Zhiwei Liu",
      "Jianguo Zhang",
      "Haolin Chen",
      "Heng Ji",
      "Weiran Yao",
      "Shelby Heinecke",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability."
  },
  {
    "url": "http://arxiv.org/abs/2507.22025v2",
    "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and   Precise Inference-Time Grounding",
    "authors": [
      "Shuquan Lian",
      "Yuhang Wu",
      "Jia Ma",
      "Zihan Song",
      "Bingqi Chen",
      "Xiawu Zheng",
      "Hui Li"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a \"Simple Thinking\" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro."
  },
  {
    "url": "http://arxiv.org/abs/2507.21980v1",
    "title": "Predicting Microbial Ontology and Pathogen Risk from Environmental   Metadata with Large Language Models",
    "authors": [
      "Hyunwoo Yoo",
      "Gail L. Rosen"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models (LLMs) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that LLMs not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that LLMs can effectively reason over sparse, heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications."
  },
  {
    "url": "http://arxiv.org/abs/2507.21934v1",
    "title": "Culinary Crossroads: A RAG Framework for Enhancing Diversity in   Cross-Cultural Recipe Adaptation",
    "authors": [
      "Tianyi Hu",
      "Andrea Morales-Garz\u00f3n",
      "Jingyi Zheng",
      "Maria Maistro",
      "Daniel Hershcovich"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs."
  },
  {
    "url": "http://arxiv.org/abs/2507.21931v1",
    "title": "Post-Training Large Language Models via Reinforcement Learning from   Self-Feedback",
    "authors": [
      "Carel van Niekerk",
      "Renato Vukovic",
      "Benjamin Matthias Ruppik",
      "Hsien-chin Lin",
      "Milica Ga\u0161i\u0107"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards."
  },
  {
    "url": "http://arxiv.org/abs/2507.21919v2",
    "title": "Training language models to be warm and empathetic makes them less   reliable and more sycophantic",
    "authors": [
      "Lujain Ibrahim",
      "Franziska Sofia Hafner",
      "Luc Rocher"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction."
  },
  {
    "url": "http://arxiv.org/abs/2507.21914v1",
    "title": "Rote Learning Considered Useful: Generalizing over Memorized Data in   LLMs",
    "authors": [
      "Qinyuan Wu",
      "Soumi Das",
      "Mahsa Amani",
      "Bishwamittra Ghosh",
      "Mohammad Aflah Khan",
      "Krishna P. Gummadi",
      "Muhammad Bilal Zafar"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Rote learning is a memorization technique based on repetition. It is commonly believed to hinder generalization by encouraging verbatim memorization rather than deeper understanding. This insight holds for even learning factual knowledge that inevitably requires a certain degree of memorization. In this work, we demonstrate that LLMs can be trained to generalize from rote memorized data. We introduce a two-phase memorize-then-generalize framework, where the model first rote memorizes factual subject-object associations using a semantically meaningless token and then learns to generalize by fine-tuning on a small set of semantically meaningful prompts. Extensive experiments over 8 LLMs show that the models can reinterpret rote memorized data through the semantically meaningful prompts, as evidenced by the emergence of structured, semantically aligned latent representations between the two. This surprising finding opens the door to both effective and efficient knowledge injection and possible risks of repurposing the memorized data for malicious usage."
  },
  {
    "url": "http://arxiv.org/abs/2507.21903v2",
    "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,   Events and Time for Timeline Generation",
    "authors": [
      "Tiviatis Sim",
      "Kaiwen Yang",
      "Shen Xin",
      "Kenji Kawaguchi"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article."
  },
  {
    "url": "http://arxiv.org/abs/2507.21892v1",
    "title": "Graph-R1: Towards Agentic GraphRAG Framework via End-to-end   Reinforcement Learning",
    "authors": [
      "Haoran Luo",
      "Haihong E",
      "Guanting Chen",
      "Qika Lin",
      "Yikai Guo",
      "Fangzhi Xu",
      "Zemin Kuang",
      "Meina Song",
      "Xiaobao Wu",
      "Yifan Zhu",
      "Luu Anh Tuan"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality."
  },
  {
    "url": "http://arxiv.org/abs/2507.21836v1",
    "title": "AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement   Learning",
    "authors": [
      "Yifan Wei",
      "Xiaoyan Yu",
      "Yixuan Weng",
      "Tengfei Pan",
      "Angsheng Li",
      "Li Du"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.21831v1",
    "title": "Introducing HALC: A general pipeline for finding optimal prompting   strategies for automated coding with LLMs in the computational social   sciences",
    "authors": [
      "Andreas Reich",
      "Claudia Thoms",
      "Tobias Schrimpf"
    ],
    "date": "2025-07-29",
    "abstract": "Title:Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences"
  },
  {
    "url": "http://arxiv.org/abs/2507.21828v1",
    "title": "Modelling Adjectival Modification Effects on Semantic Plausibility",
    "authors": [
      "Anna Golub",
      "Beate Zywietz",
      "Annerose Eichel"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:While the task of assessing the plausibility of events such as ''news is relevant'' has been addressed by a growing body of work, less attention has been paid to capturing changes in plausibility as triggered by event modification. Understanding changes in plausibility is relevant for tasks such as dialogue generation, commonsense reasoning, and hallucination detection as it allows to correctly model, for example, ''gentle sarcasm'' as a sign of closeness rather than unkindness among friends [9]. In this work, we tackle the ADEPT challenge benchmark [6] consisting of 16K English sentence pairs differing by exactly one adjectival modifier. Our modeling experiments provide a conceptually novel method by using sentence transformers, and reveal that both they and transformer-based models struggle with the task at hand, and sentence transformers - despite their conceptual alignment with the task - even under-perform in comparison to models like RoBERTa. Furthermore, an in-depth comparison with prior work highlights the importance of a more realistic, balanced evaluation method: imbalances distort model performance and evaluation metrics, and weaken result trustworthiness."
  },
  {
    "url": "http://arxiv.org/abs/2507.21815v1",
    "title": "HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to   Support People Who Use Drugs",
    "authors": [
      "Kaixuan Wang",
      "Chenxin Diao",
      "Jason T. Jacques",
      "Zhongliang Guo",
      "Shuai Zhao"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms."
  },
  {
    "url": "http://arxiv.org/abs/2507.21813v1",
    "title": "Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in   Spanish",
    "authors": [
      "Elena Alvarez-Mellado",
      "Jordi Porta-Zamorano",
      "Constantine Lignos",
      "Julio Gonzalo"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:This paper summarizes the main findings of ADoBo 2025, the shared task on anglicism identification in Spanish proposed in the context of IberLEF 2025. Participants of ADoBo 2025 were asked to detect English lexical borrowings (or anglicisms) from a collection of Spanish journalistic texts. Five teams submitted their solutions for the test phase. Proposed systems included LLMs, deep learning models, Transformer-based models and rule-based systems. The results range from F1 scores of 0.17 to 0.99, which showcases the variability in performance different systems can have for this task."
  },
  {
    "url": "http://arxiv.org/abs/2507.21810v1",
    "title": "ChartMark: A Structured Grammar for Chart Annotation",
    "authors": [
      "Yiyu Chen",
      "Yifan Wu",
      "Shuyu Shen",
      "Yupeng Xie",
      "Leixian Shen",
      "Hui Xiong",
      "Yuyu Luo"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability."
  },
  {
    "url": "http://arxiv.org/abs/2507.21782v1",
    "title": "The Problem with Safety Classification is not just the Models",
    "authors": [
      "Sowmya Vajjala"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:Studying the robustness of Large Language Models (LLMs) to unsafe behaviors is an important topic of research today. Building safety classification models or guard models, which are fine-tuned models for input/output safety classification for LLMs, is seen as one of the solutions to address the issue. Although there is a lot of research on the safety testing of LLMs themselves, there is little research on evaluating the effectiveness of such safety classifiers or the evaluation datasets used for testing them, especially in multilingual scenarios. In this position paper, we demonstrate how multilingual disparities exist in 5 safety classification models by considering datasets covering 18 languages. At the same time, we identify potential issues with the evaluation datasets, arguing that the shortcomings of current safety classifiers are not only because of the models themselves. We expect that these findings will contribute to the discussion on developing better methods to identify harmful content in LLM inputs across languages."
  },
  {
    "url": "http://arxiv.org/abs/2507.21773v1",
    "title": "AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large   Language Models",
    "authors": [
      "Lian Yan",
      "Haotian Wang",
      "Chen Tang",
      "Haifeng Liu",
      "Tianyang Sun",
      "Liangliang Liu",
      "Yi Guan",
      "Jingchi Jiang"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.21750v1",
    "title": "Adversarial Defence without Adversarial Defence: Enhancing Language   Model Robustness via Instance-level Principal Component Removal",
    "authors": [
      "Yang Wang",
      "Chenghao Xiao",
      "Yizhi Li",
      "Stuart E. Middleton",
      "Noura Al Moubayed",
      "Chenghua Lin"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation."
  },
  {
    "url": "http://arxiv.org/abs/2507.21652v1",
    "title": "UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases",
    "authors": [
      "Raj Vardhan Tomar",
      "Preslav Nakov",
      "Yuxia Wang"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at this https URL"
  },
  {
    "url": "http://arxiv.org/abs/2507.21645v1",
    "title": "Libra: Assessing and Improving Reward Model by Learning to Think",
    "authors": [
      "Meng Zhou",
      "Bei Li",
      "Jiahao Liu",
      "Xiaowen Shi",
      "Yang Bai",
      "Rongxiang Weng",
      "Jingang Wang",
      "Xunliang Cai"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data."
  },
  {
    "url": "http://arxiv.org/abs/2507.21609v1",
    "title": "Multilingual JobBERT for Cross-Lingual Job Title Matching",
    "authors": [
      "Jens-Joris Decorte",
      "Matthias De Lange",
      "Jeroen Van Hautte"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual job title matching. Building on the state-of-the-art monolingual JobBERT-V2, our approach extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a balanced multilingual dataset of over 21 million job titles. The model retains the efficiency-focused architecture of its predecessor while enabling robust alignment across languages without requiring task-specific supervision. Extensive evaluations on the TalentCLEF 2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual baselines and achieves consistent performance across both monolingual and cross-lingual settings. While not the primary focus, we also show that the model can be effectively used to rank relevant skills for a given job title, demonstrating its broader applicability in multilingual labor market intelligence. The model is publicly available: this https URL."
  },
  {
    "url": "http://arxiv.org/abs/2507.21568v2",
    "title": "Multi-Hypothesis Distillation of Multilingual Neural Translation Models   for Low-Resource Languages",
    "authors": [
      "Aar\u00f3n Galiano-Jim\u00e9nez",
      "Juan Antonio P\u00e9rez-Ortiz",
      "Felipe S\u00e1nchez-Mart\u00ednez",
      "V\u00edctor M. S\u00e1nchez-Cartagena"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:This paper explores sequence-level knowledge distillation (KD) of multilingual pre-trained encoder-decoder translation models. We argue that the teacher model's output distribution holds valuable insights for the student, beyond the approximated mode obtained through beam search (the standard decoding method), and present Multi-Hypothesis Distillation (MHD), a sequence-level KD method that generates multiple translations for each source sentence. This provides a larger representation of the teacher model distribution and exposes the student model to a wider range of target-side prefixes. We leverage $n$-best lists from beam search to guide the student's learning and examine alternative decoding methods to address issues like low variability and the under-representation of infrequent tokens. For low-resource languages, our research shows that while sampling methods may slightly compromise translation quality compared to beam search based approaches, they enhance the generated corpora with greater variability and lexical richness. This ultimately improves student model performance and mitigates the gender bias amplification often associated with KD."
  },
  {
    "url": "http://arxiv.org/abs/2507.21556v1",
    "title": "Evaluating the cognitive reality of Spanish irregular morphomic   patterns: Humans vs. Transformers",
    "authors": [
      "Akhilesh Kakolu Ramarao",
      "Kevin Tang",
      "Dinah Baer-Henney"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:This study investigates the cognitive plausibility of the Spanish irregular morphomic pattern by directly comparing transformer-based neural networks to human behavioral data from \\citet{Nevins2015TheRA}. Using the same analytical framework as the original human study, we evaluate whether transformer models can replicate human-like sensitivity to a complex linguistic phenomena, the morphome, under controlled input conditions. Our experiments focus on three frequency conditions: natural, low-frequency, and high-frequency distributions of verbs exhibiting irregular morphomic patterns. While the models outperformed humans in stem and suffix accuracy, a clear divergence emerged in response preferences. Unlike humans, who consistently favored natural responses across all test items, models' preferred irregular responses and were influenced by the proportion of irregular verbs in their training data. Additionally, models trained on the natural and low-frequency distributions, but not the high-frequency distribution, were sensitive to the phonological similarity between test items and real Spanish L-shaped verbs."
  },
  {
    "url": "http://arxiv.org/abs/2507.21544v1",
    "title": "MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts   in Retrieval-Augmented Generation",
    "authors": [
      "Jungyeon Lee",
      "Kangmin Lee",
      "Taeuk Kim"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information."
  },
  {
    "url": "http://arxiv.org/abs/2507.21536v1",
    "title": "Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic   Framework for a Low-Resource Language",
    "authors": [
      "Jiaxin Zuo",
      "Yiquan Wang",
      "Yuan Pan",
      "Xiadiya Yibulayin"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:To address a critical resource gap in Uyghur Natural Language Processing (NLP), this study introduces a dependency annotation framework designed to overcome the limitations of existing treebanks for the low-resource, agglutinative language. This inventory includes 18 main relations and 26 subtypes, with specific labels such as cop:zero for verbless clauses and instr:case=loc/dat for nuanced instrumental functions. To empirically validate the necessity of this tailored approach, we conducted a cross-standard evaluation using a pre-trained Universal Dependencies parser. The analysis revealed a systematic 47.9% divergence in annotations, pinpointing the inadequacy of universal schemes for handling Uyghur-specific structures. Grounded in nine annotation principles that ensure typological accuracy and semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a more accurate and semantically transparent representation, designed to enable significant improvements in parsing and downstream NLP tasks, and offers a replicable model for other morphologically complex languages."
  },
  {
    "url": "http://arxiv.org/abs/2507.21532v1",
    "title": "Automatic Classification of User Requirements from Online Feedback -- A   Replication Study",
    "authors": [
      "Meet Bhatt",
      "Nic Boilard",
      "Muhammad Rehan Chaudhary",
      "Cole Thompson",
      "Jacob Idoko",
      "Aakash Sorathiya",
      "Gouri Ginde"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), \"Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning\", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study."
  },
  {
    "url": "http://arxiv.org/abs/2507.21526v1",
    "title": "TriangleMix: A Lossless and Efficient Attention Pattern for Long Context   Prefilling",
    "authors": [
      "Zhiyuan He",
      "Yike Zhang",
      "Chengruidong Zhang",
      "Huiqiang Jiang",
      "Yuqing Yang",
      "Lili Qiu"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static sparse attention methods typically degrade accuracy, while dynamic sparsity methods introduce additional computational overhead due to runtime sparse index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance LLM inference efficiency."
  },
  {
    "url": "http://arxiv.org/abs/2507.21522v1",
    "title": "Model-free Speculative Decoding for Transformer-based ASR with Token Map   Drafting",
    "authors": [
      "Tuan Vu Ho",
      "Hiroaki Kokubo",
      "Masaaki Yamamoto",
      "Yohei Kawaguchi"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:End-to-end automatic speech recognition (ASR) systems based on transformer architectures, such as Whisper, offer high transcription accuracy and robustness. However, their autoregressive decoding is computationally expensive, hence limiting deployment on CPU-based and resource-constrained devices. Speculative decoding (SD) mitigates this issue by using a smaller draft model to propose candidate tokens, which are then verified by the main model. However, this approach is impractical for devices lacking hardware accelerators like GPUs. To address this, we propose \\emph{Token Map Drafting}, a model-free SD technique that eliminates the need for a separate draft model. Instead, we leverage a precomputed n-gram token map derived from domain-specific training data, enabling efficient speculative decoding with minimal overhead. Our method significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results demonstrate decoding speed-ups of $1.27\\times$ on the CI-AVSR dataset and $1.37\\times$ on our internal dataset without degrading recognition accuracy. Additionally, our approach achieves a $10\\%$ absolute improvement in decoding speed over the Distill-spec baseline running on CPU, highlighting its effectiveness for on-device ASR applications."
  },
  {
    "url": "http://arxiv.org/abs/2507.21513v1",
    "title": "What Does it Mean for a Neural Network to Learn a \"World Model\"?",
    "authors": [
      "Kenneth Li",
      "Fernanda Vi\u00e9gas",
      "Martin Wattenberg"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:We propose a set of precise criteria for saying a neural net learns and uses a \"world model.\" The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent \"state space\" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a \"world model\" is not a trivial consequence of the neural net's data or task."
  },
  {
    "url": "http://arxiv.org/abs/2507.21509v1",
    "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language   Models",
    "authors": [
      "Runjin Chen",
      "Andy Arditi",
      "Henry Sleight",
      "Owain Evans",
      "Jack Lindsey"
    ],
    "date": "2025-07-29",
    "abstract": "View PDFAbstract:Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description."
  },
  {
    "url": "http://arxiv.org/abs/2507.21500v1",
    "title": "VN-MTEB: Vietnamese Massive Text Embedding Benchmark",
    "authors": [
      "Loc Pham",
      "Tung Luu",
      "Thu Vo",
      "Minh Nguyen",
      "Viet Hoang"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: this https URL"
  },
  {
    "url": "http://arxiv.org/abs/2507.21482v1",
    "title": "Improving Task Diversity in Label Efficient Supervised Finetuning of   LLMs",
    "authors": [
      "Abhinav Arabelly",
      "Jagrut Nemade",
      "Robert D Nowak",
      "Jifan Zhang"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\\%."
  },
  {
    "url": "http://arxiv.org/abs/2507.21476v1",
    "title": "Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with   HumorBench",
    "authors": [
      "Reuben Narad",
      "Siddharth Suresh",
      "Jiayi Chen",
      "Pine S. L. Dysart-Bricken",
      "Bob Mankoff",
      "Robert Nowak",
      "Jifan Zhang",
      "Lalit Jain"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and this http URL, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning."
  },
  {
    "url": "http://arxiv.org/abs/2507.21432v1",
    "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for   Mode Choice Behaviour",
    "authors": [
      "Tareq Alsaleh",
      "Bilal Farooq"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment."
  },
  {
    "url": "http://arxiv.org/abs/2507.21428v1",
    "title": "MemTool: Optimizing Short-Term Memory Management for Dynamic Tool   Calling in LLM Agent Multi-Turn Conversations",
    "authors": [
      "Elias Lumer",
      "Anmol Gulati",
      "Vamse Kumar Subbiah",
      "Pradeep Honaganahalli Basavaraju",
      "James A. Burke"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities."
  },
  {
    "url": "http://arxiv.org/abs/2507.21420v1",
    "title": "ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs",
    "authors": [
      "Chaoyu Li",
      "Yogesh Kulkarni",
      "Pooyan Fazli"
    ],
    "date": "2025-07-29",
    "abstract": "View PDF HTML (experimental)Abstract:The computational cost of training multimodal large language models (MLLMs) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student framework in which the MLLM being trained serves as the student, and a frozen reference large language model (LLM) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon."
  }
]