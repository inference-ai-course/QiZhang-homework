{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2232c869-0721-4682-acb1-26aed8831a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading documents...\n",
      "‚ö†Ô∏è Skipping malformed line in Task 3: Expecting value: line 1 column 1 (char 0)\n",
      "‚úÖ Loaded 274 documents from Tasks 1‚Äì3\n",
      "üåê Detecting language...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language filtering: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [00:01<00:00, 146.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Removing HTML noise...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stripping HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259/259 [00:00<00:00, 5558.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåÄ Deduplicating documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplication: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259/259 [00:00<00:00, 335.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Removing PII...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PII removal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259/259 [00:00<00:00, 2563.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ôªÔ∏è Removing repetitive n-grams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repetition cleanup: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259/259 [00:00<00:00, 3225.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaning complete. Saved corpus to task4/clean_corpus.txt and stats to task4/stats.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from langdetect import detect\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ------------ Config ------------\n",
    "INPUT_JSON1 = \"arxiv_clean.json\"   # Task 1\n",
    "INPUT_TXT2 = \"pdf_text/*.txt\"            # Task 2\n",
    "INPUT_JSONL3 = \"task3/talks_transcripts.jsonl\" # Task 3\n",
    "OUTPUT_CORPUS = \"task4/clean_corpus.txt\"\n",
    "OUTPUT_STATS = \"task4/stats.md\"\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "TOP_N_WORDS = 20\n",
    "\n",
    "# ------------ Step 1: Load Data ------------\n",
    "documents = []\n",
    "\n",
    "print(\"üì• Loading documents...\")\n",
    "\n",
    "# Task 1\n",
    "if os.path.exists(INPUT_JSON1):\n",
    "    with open(INPUT_JSON1, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            papers = json.load(f)\n",
    "            for p in papers:\n",
    "                documents.append(p[\"title\"] + \"\\n\" + p.get(\"abstract\", \"\"))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Could not parse Task 1 JSON file, skipping...\")\n",
    "\n",
    "# Task 2\n",
    "for file in glob(INPUT_TXT2):\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            documents.append(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not read {file}: {e}\")\n",
    "\n",
    "# Task 3\n",
    "if os.path.exists(INPUT_JSONL3):\n",
    "    with open(INPUT_JSONL3, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                documents.append(rec.get(\"speaker_text\", \"\") + \"\\n\" + rec.get(\"ocr_text\", \"\"))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping malformed line in Task 3: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents from Tasks 1‚Äì3\")\n",
    "\n",
    "# ------------ Step 2: Language Detection (keep English) ------------\n",
    "print(\"üåê Detecting language...\")\n",
    "docs_lang_filtered = []\n",
    "for d in tqdm(documents, desc=\"Language filtering\"):\n",
    "    try:\n",
    "        if detect(d) == \"en\":\n",
    "            docs_lang_filtered.append(d)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# ------------ Step 3: Strip HTML Noise ------------\n",
    "def clean_html(text):\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"&\\w+;\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "print(\"üßπ Removing HTML noise...\")\n",
    "docs_no_html = [clean_html(d) for d in tqdm(docs_lang_filtered, desc=\"Stripping HTML\")]\n",
    "\n",
    "# ------------ Step 4: Deduplication with MinHash ------------\n",
    "print(\"üåÄ Deduplicating documents...\")\n",
    "lsh = MinHashLSH(threshold=SIMILARITY_THRESHOLD, num_perm=128)\n",
    "unique_docs = []\n",
    "\n",
    "for i, doc in enumerate(tqdm(docs_no_html, desc=\"Deduplication\")):\n",
    "    mh = MinHash(num_perm=128)\n",
    "    for word in set(doc.split()):\n",
    "        mh.update(word.encode(\"utf8\"))\n",
    "    if not lsh.query(mh):\n",
    "        lsh.insert(f\"doc{i}\", mh)\n",
    "        unique_docs.append(doc)\n",
    "\n",
    "# ------------ Step 5: Remove PII ------------\n",
    "def remove_pii(text):\n",
    "    text = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \"[EMAIL]\", text)\n",
    "    text = re.sub(r\"\\b(?:\\d[ -]*?){13,16}\\b\", \"[CREDIT_CARD]\", text)\n",
    "    text = re.sub(r\"\\+?\\d[\\d -]{8,}\\d\", \"[PHONE]\", text)\n",
    "    return text\n",
    "\n",
    "print(\"üîí Removing PII...\")\n",
    "docs_no_pii = [remove_pii(d) for d in tqdm(unique_docs, desc=\"PII removal\")]\n",
    "\n",
    "# ------------ Step 6: Remove repetitive n-grams ------------\n",
    "def remove_repetitions(text, n=3):\n",
    "    tokens = text.split()\n",
    "    cleaned, seen = [], set()\n",
    "    for i in range(len(tokens)):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        if ngram in seen:\n",
    "            continue\n",
    "        seen.add(ngram)\n",
    "        cleaned.append(tokens[i])\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "print(\"‚ôªÔ∏è Removing repetitive n-grams...\")\n",
    "docs_cleaned = [remove_repetitions(d) for d in tqdm(docs_no_pii, desc=\"Repetition cleanup\")]\n",
    "\n",
    "# ------------ Step 7: Save Output ------------\n",
    "with open(OUTPUT_CORPUS, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in docs_cleaned:\n",
    "        f.write(d + \"\\n\")\n",
    "\n",
    "# ------------ Step 8: Stats ------------\n",
    "original_tokens = sum(len(d.split()) for d in documents)\n",
    "cleaned_tokens = sum(len(d.split()) for d in docs_cleaned)\n",
    "removed_docs = len(documents) - len(docs_cleaned)\n",
    "\n",
    "# Word frequency (top N)\n",
    "word_counts = Counter(\" \".join(docs_cleaned).split())\n",
    "top_words = word_counts.most_common(TOP_N_WORDS)\n",
    "\n",
    "with open(OUTPUT_STATS, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Cleaning Statistics\\n\")\n",
    "    f.write(f\"- Original documents: {len(documents)}\\n\")\n",
    "    f.write(f\"- Cleaned documents: {len(docs_cleaned)}\\n\")\n",
    "    f.write(f\"- Removed documents: {removed_docs} ({removed_docs/len(documents)*100:.2f}%)\\n\")\n",
    "    f.write(f\"- Original tokens: {original_tokens}\\n\")\n",
    "    f.write(f\"- Cleaned tokens: {cleaned_tokens}\\n\\n\")\n",
    "    f.write(\"## Top Frequent Words\\n\")\n",
    "    f.write(\"| Word | Count |\\n|------|-------|\\n\")\n",
    "    for word, count in top_words:\n",
    "        f.write(f\"| {word} | {count} |\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cleaning complete. Saved corpus to {OUTPUT_CORPUS} and stats to {OUTPUT_STATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffb842-61d8-4867-a877-3a94e1374547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Week2Homework)",
   "language": "python",
   "name": "week2homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
